#!/usr/bin/env python3
"""
AUTUS Decision Log
==================
ì‚¶ì˜ ëª¨ë“  ê²°ì •ì„ ë¬¼ë¦¬ëŸ‰ìœ¼ë¡œ ê¸°ë¡

í•µì‹¬ ì›ì¹™:
1. ëª¨ë“  ê²°ì •ì€ Before â†’ Action â†’ After (Decision Trinity)
2. ê°ì •/ì˜ê²¬ ë°°ì œ, ì˜¤ì§ ë²¡í„°ë§Œ ê¸°ë¡
3. ì˜ˆì¸¡ê³¼ ì‹¤ì œì˜ ì°¨ì´(Îµ)ë¥¼ ëˆ„ì  í•™ìŠµ
4. ì‚¶ì˜ ê·¼ë³¸ ë°©ì •ì‹: dS/dt = F - R - Î»H
"""

import time
import uuid
import json
import math
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from enum import Enum
from datetime import datetime


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê¸°ë³¸ ë¬¼ë¦¬ëŸ‰ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class StateVector:
    """
    ìƒíƒœ ë²¡í„° S(t) = [Energy, Flow, Risk]
    
    - Energy: ìì› (ì‹œê°„, ëˆ, ê±´ê°•, ê´€ê³„) [0, 1]
    - Flow: íë¦„/ì§„í–‰ ì†ë„ [0, 1]
    - Risk: ë¦¬ìŠ¤í¬/ë¶ˆí™•ì‹¤ì„± [0, 1]
    """
    energy: float
    flow: float
    risk: float
    
    def __post_init__(self):
        # ë²”ìœ„ ì œí•œ
        self.energy = max(0.0, min(1.0, self.energy))
        self.flow = max(0.0, min(1.0, self.flow))
        self.risk = max(0.0, min(1.0, self.risk))
    
    def to_list(self) -> List[float]:
        return [self.energy, self.flow, self.risk]
    
    def magnitude(self) -> float:
        """ìƒíƒœ ë²¡í„°ì˜ í¬ê¸° (ì „ì²´ ìƒíƒœ ì ìˆ˜)"""
        # Energyì™€ FlowëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ê³ , RiskëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        return math.sqrt(self.energy**2 + self.flow**2 + (1-self.risk)**2) / math.sqrt(3)
    
    def __sub__(self, other: 'StateVector') -> 'StateVector':
        """ë‘ ìƒíƒœì˜ ì°¨ì´ (Î”)"""
        return StateVector(
            energy=self.energy - other.energy,
            flow=self.flow - other.flow,
            risk=self.risk - other.risk
        )
    
    def __add__(self, other: 'StateVector') -> 'StateVector':
        return StateVector(
            energy=self.energy + other.energy,
            flow=self.flow + other.flow,
            risk=self.risk + other.risk
        )
    
    def to_dict(self) -> Dict:
        return {
            "energy": round(self.energy, 4),
            "flow": round(self.flow, 4),
            "risk": round(self.risk, 4),
            "magnitude": round(self.magnitude(), 4)
        }


class ActionType(Enum):
    """í–‰ë™ ìœ í˜• (í˜ì˜ ë°©í–¥)"""
    THROTTLE = "throttle"   # ê°€ì† - ì—ë„ˆì§€ íˆ¬ì…í•˜ì—¬ ì „ì§„
    DETOUR = "detour"       # ìš°íšŒ - ì €í•­ íšŒí”¼
    BRAKE = "brake"         # ê°ì† - ë¦¬ìŠ¤í¬ ê°ì†Œ
    PAUSE = "pause"         # ì •ì§€ - ì—ë„ˆì§€ ë³´ì¡´
    PIVOT = "pivot"         # ì „í™˜ - ë°©í–¥ ë³€ê²½


@dataclass
class ForceVector:
    """
    í˜ ë²¡í„° F = (type, magnitude, direction)
    
    - type: í–‰ë™ ìœ í˜•
    - magnitude: í˜ì˜ í¬ê¸° [0, 1]
    - direction: ë°©í–¥ ë²¡í„° (Energy, Flow, Risk ë°©í–¥)
    """
    action_type: ActionType
    magnitude: float
    direction: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    
    def __post_init__(self):
        self.magnitude = max(0.0, min(1.0, self.magnitude))
        
        # ê¸°ë³¸ ë°©í–¥ ì„¤ì • (í–‰ë™ ìœ í˜•ë³„)
        if self.direction == (0.0, 0.0, 0.0):
            self.direction = {
                ActionType.THROTTLE: (0.3, 0.6, 0.1),   # Flow ì¤‘ì‹¬
                ActionType.DETOUR: (0.2, 0.3, -0.5),    # Risk ê°ì†Œ
                ActionType.BRAKE: (-0.1, -0.2, -0.7),   # Risk ëŒ€í­ ê°ì†Œ
                ActionType.PAUSE: (0.1, -0.3, -0.2),    # ì—ë„ˆì§€ ë³´ì¡´
                ActionType.PIVOT: (0.0, 0.0, 0.0),      # ë°©í–¥ ì¬ì„¤ì •
            }.get(self.action_type, (0.0, 0.0, 0.0))
    
    def apply_to_state(self, state: StateVector, resistance: float = 0.1) -> StateVector:
        """í˜ì„ ìƒíƒœì— ì ìš© â†’ ìƒˆë¡œìš´ ìƒíƒœ"""
        effective = self.magnitude * (1 - resistance)
        
        return StateVector(
            energy=state.energy + self.direction[0] * effective,
            flow=state.flow + self.direction[1] * effective,
            risk=state.risk + self.direction[2] * effective
        )
    
    def to_dict(self) -> Dict:
        return {
            "type": self.action_type.value,
            "magnitude": round(self.magnitude, 4),
            "direction": [round(d, 4) for d in self.direction]
        }


@dataclass
class Intention:
    """
    ì˜ë„ (ëª©í‘œ ìƒíƒœ)
    - goal_state: ë„ë‹¬í•˜ê³ ì í•˜ëŠ” ìƒíƒœ
    - urgency: ê¸´ê¸‰ë„ [0, 1]
    """
    goal_state: StateVector
    urgency: float = 0.5
    description: str = ""  # ì„ íƒì  ë©”ëª¨ (ë¬¼ë¦¬ëŸ‰ ì•„ë‹˜)
    
    def distance_from(self, current: StateVector) -> float:
        """í˜„ì¬ ìƒíƒœì—ì„œ ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬"""
        diff = self.goal_state - current
        return math.sqrt(diff.energy**2 + diff.flow**2 + diff.risk**2)
    
    def to_dict(self) -> Dict:
        return {
            "goal": self.goal_state.to_dict(),
            "urgency": round(self.urgency, 4),
            "distance": None  # ê³„ì‚° ì‹œ ì±„ì›€
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Decision Log (ê²°ì • ê¸°ë¡)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class PhysicsMetrics:
    """ë¬¼ë¦¬ ë©”íŠ¸ë¦­ìŠ¤ - ê²°ì •ì˜ íš¨ìœ¨ì„± ì¸¡ì •"""
    efficiency: float = 0.0        # íš¨ìœ¨ (ì‹¤ì œÎ” / ì˜ˆìƒÎ”)
    resistance_actual: float = 0.0 # ì‹¤ì œ ì €í•­
    entropy_loss: float = 0.0      # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
    prediction_error: float = 0.0  # ì˜ˆì¸¡ ì˜¤ì°¨ (Îµ)
    
    def to_dict(self) -> Dict:
        return {
            "efficiency": round(self.efficiency, 4),
            "resistance": round(self.resistance_actual, 4),
            "entropy_loss": round(self.entropy_loss, 4),
            "prediction_error": round(self.prediction_error, 4)
        }


@dataclass
class DecisionLog:
    """
    ê²°ì • ë¡œê·¸ - Decision Trinity
    
    â‘  BEFORE: ê²°ì • ì „ ìƒíƒœ
    â‘¡ ACTION: ì„ íƒí•œ í–‰ë™ (í˜)
    â‘¢ AFTER: ê²°ì • í›„ ìƒíƒœ + Î” + ë¬¼ë¦¬ ë©”íŠ¸ë¦­ìŠ¤
    """
    # ì‹ë³„ì
    decision_id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    timestamp: float = field(default_factory=time.time)
    
    # â‘  BEFORE
    state_before: Optional[StateVector] = None
    momentum: Optional[StateVector] = None  # ë³€í™” ì¶”ì„¸
    intention: Optional[Intention] = None
    
    # â‘¡ ACTION
    action: Optional[ForceVector] = None
    context: Dict = field(default_factory=dict)  # ë¹„ë¬¼ë¦¬ì  ì»¨í…ìŠ¤íŠ¸ (ìµœì†Œí™”)
    
    # â‘¢ AFTER
    state_after: Optional[StateVector] = None
    delta: Optional[StateVector] = None
    delta_t: float = 0.0  # ì†Œìš” ì‹œê°„ (ì´ˆ)
    
    # ë¬¼ë¦¬ ë©”íŠ¸ë¦­ìŠ¤
    physics: PhysicsMetrics = field(default_factory=PhysicsMetrics)
    
    # ì˜ˆì¸¡ê°’ (ì‚¬ì „)
    predicted_delta: Optional[StateVector] = None
    
    def complete(self, state_after: StateVector, delta_t: float):
        """ê²°ì • ì™„ë£Œ - AFTER ìƒíƒœ ê¸°ë¡ ë° ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        self.state_after = state_after
        self.delta_t = delta_t
        
        if self.state_before:
            self.delta = state_after - self.state_before
        
        # ë¬¼ë¦¬ ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        if self.predicted_delta and self.delta:
            pred_mag = math.sqrt(sum(x**2 for x in self.predicted_delta.to_list()))
            actual_mag = math.sqrt(sum(x**2 for x in self.delta.to_list()))
            
            if pred_mag > 0:
                self.physics.efficiency = actual_mag / pred_mag
            
            # ì˜ˆì¸¡ ì˜¤ì°¨
            error = self.delta - self.predicted_delta
            self.physics.prediction_error = math.sqrt(sum(x**2 for x in error.to_list()))
        
        # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ (ì‹œê°„ì— ë¹„ë¡€)
        self.physics.entropy_loss = 0.001 * delta_t  # Î» = 0.001
    
    def to_dict(self) -> Dict:
        return {
            "decision_id": self.decision_id,
            "ts": self.timestamp,
            "ts_human": datetime.fromtimestamp(self.timestamp).isoformat(),
            
            "before": {
                "state": self.state_before.to_dict() if self.state_before else None,
                "momentum": self.momentum.to_dict() if self.momentum else None
            },
            
            "intention": self.intention.to_dict() if self.intention else None,
            
            "action": self.action.to_dict() if self.action else None,
            
            "after": {
                "state": self.state_after.to_dict() if self.state_after else None,
                "delta": self.delta.to_dict() if self.delta else None,
                "delta_t": round(self.delta_t, 2)
            },
            
            "physics": self.physics.to_dict()
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Decision Engine (ê²°ì • ì—”ì§„)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LifePhysicsEngine:
    """
    ì‚¶ì˜ ë¬¼ë¦¬ ì—”ì§„
    
    - ìƒíƒœ ì˜ˆì¸¡
    - í•„ìš”í•œ í˜ ê³„ì‚°
    - ê²°ì • ì‹œë®¬ë ˆì´ì…˜
    """
    
    # ë¬¼ë¦¬ ìƒìˆ˜
    ENTROPY_DECAY = 0.001  # Î»: ì´ˆë‹¹ ì—”íŠ¸ë¡œí”¼ ì¦ê°€ìœ¨
    BASE_RESISTANCE = 0.1  # ê¸°ë³¸ ì €í•­
    
    def __init__(self):
        self.decision_history: List[DecisionLog] = []
        self.current_state = StateVector(0.5, 0.5, 0.5)
    
    def predict_state(
        self, 
        current: StateVector, 
        action: ForceVector, 
        delta_t: float,
        resistance: float = None
    ) -> StateVector:
        """
        ìƒíƒœ ì˜ˆì¸¡: dS/dt = F - R - Î»H
        
        Args:
            current: í˜„ì¬ ìƒíƒœ
            action: ì ìš©í•  í˜
            delta_t: ì‹œê°„ ê°„ê²© (ì´ˆ)
            resistance: ì €í•­ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
        
        Returns:
            ì˜ˆì¸¡ ìƒíƒœ
        """
        if resistance is None:
            resistance = self.BASE_RESISTANCE
        
        # í˜ ì ìš©
        new_state = action.apply_to_state(current, resistance)
        
        # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ (Risk ìì—° ì¦ê°€)
        entropy_loss = self.ENTROPY_DECAY * delta_t
        new_state.risk = min(1.0, new_state.risk + entropy_loss)
        
        return new_state
    
    def calculate_required_force(
        self,
        current: StateVector,
        goal: StateVector,
        max_time: float
    ) -> Tuple[ForceVector, float]:
        """
        í•„ìš”í•œ í˜ ê³„ì‚°
        
        Args:
            current: í˜„ì¬ ìƒíƒœ
            goal: ëª©í‘œ ìƒíƒœ
            max_time: ìµœëŒ€ í—ˆìš© ì‹œê°„ (ì´ˆ)
        
        Returns:
            (í•„ìš”í•œ í˜, ì˜ˆìƒ ë„ë‹¬ ì‹œê°„)
        """
        diff = goal - current
        distance = math.sqrt(sum(x**2 for x in diff.to_list()))
        
        # í•„ìš”í•œ í˜ í¬ê¸°
        required_magnitude = distance / (max_time * (1 - self.BASE_RESISTANCE))
        required_magnitude = min(1.0, required_magnitude)
        
        # ë°©í–¥
        if distance > 0:
            direction = (
                diff.energy / distance,
                diff.flow / distance,
                diff.risk / distance
            )
        else:
            direction = (0.0, 0.0, 0.0)
        
        # í–‰ë™ ìœ í˜• ê²°ì •
        if diff.flow > abs(diff.energy) and diff.flow > abs(diff.risk):
            action_type = ActionType.THROTTLE
        elif diff.risk < -0.1:
            action_type = ActionType.BRAKE
        elif diff.energy < -0.1:
            action_type = ActionType.PAUSE
        else:
            action_type = ActionType.DETOUR
        
        force = ForceVector(
            action_type=action_type,
            magnitude=required_magnitude,
            direction=direction
        )
        
        # ì˜ˆìƒ ë„ë‹¬ ì‹œê°„
        if required_magnitude > 0:
            eta = distance / (required_magnitude * (1 - self.BASE_RESISTANCE))
        else:
            eta = float('inf')
        
        return force, eta
    
    def can_reach_goal(
        self,
        current: StateVector,
        goal: StateVector,
        available_force: float,
        max_time: float
    ) -> Tuple[bool, str]:
        """
        AUTUSì˜ ìœ ì¼í•œ ì§ˆë¬¸:
        "ì´ í˜(Action)ìœ¼ë¡œ ì € ìƒíƒœ(Goal)ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ê°€?"
        """
        required_force, eta = self.calculate_required_force(current, goal, max_time)
        
        if available_force >= required_force.magnitude:
            return True, f"Proceed. ETA: {eta:.1f}s, Required force: {required_force.magnitude:.2f}"
        else:
            deficit = required_force.magnitude - available_force
            return False, f"Insufficient. Required: +{deficit:.2f} force OR reduce resistance"
    
    def start_decision(
        self,
        intention: Intention,
        action: ForceVector
    ) -> DecisionLog:
        """ê²°ì • ì‹œì‘ - BEFORE + ACTION ê¸°ë¡"""
        log = DecisionLog(
            state_before=StateVector(
                self.current_state.energy,
                self.current_state.flow,
                self.current_state.risk
            ),
            intention=intention,
            action=action
        )
        
        # ì˜ˆì¸¡
        log.predicted_delta = self.predict_state(
            self.current_state, 
            action, 
            delta_t=3600  # 1ì‹œê°„ ê°€ì •
        ) - self.current_state
        
        return log
    
    def complete_decision(
        self,
        log: DecisionLog,
        actual_state: StateVector,
        delta_t: float
    ) -> DecisionLog:
        """ê²°ì • ì™„ë£Œ - AFTER ê¸°ë¡"""
        log.complete(actual_state, delta_t)
        
        # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.current_state = actual_state
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.decision_history.append(log)
        
        return log
    
    def get_learning_summary(self) -> Dict:
        """í•™ìŠµ ìš”ì•½ - ì˜ˆì¸¡ ì •í™•ë„ ë“±"""
        if not self.decision_history:
            return {"decisions": 0, "avg_efficiency": 0, "avg_error": 0}
        
        efficiencies = [d.physics.efficiency for d in self.decision_history if d.physics.efficiency > 0]
        errors = [d.physics.prediction_error for d in self.decision_history]
        
        return {
            "decisions": len(self.decision_history),
            "avg_efficiency": sum(efficiencies) / len(efficiencies) if efficiencies else 0,
            "avg_error": sum(errors) / len(errors) if errors else 0,
            "total_entropy_loss": sum(d.physics.entropy_loss for d in self.decision_history)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("=" * 70)
    print("ğŸŒŒ AUTUS Life Physics Engine - Decision Log Test")
    print("=" * 70)
    
    engine = LifePhysicsEngine()
    engine.current_state = StateVector(energy=0.72, flow=0.45, risk=0.18)
    
    print(f"\nğŸ“ Current State: {engine.current_state.to_dict()}")
    
    # ëª©í‘œ ì„¤ì •
    goal = StateVector(energy=0.85, flow=0.75, risk=0.10)
    intention = Intention(goal_state=goal, urgency=0.8, description="Q1 ë§¤ì¶œ ëª©í‘œ")
    
    print(f"ğŸ¯ Goal State: {goal.to_dict()}")
    print(f"ğŸ“ Distance to Goal: {intention.distance_from(engine.current_state):.4f}")
    
    # AUTUSì˜ ì§ˆë¬¸
    print("\n" + "-" * 70)
    print("â“ AUTUS Question: Can you reach the goal with force=0.5?")
    can_reach, message = engine.can_reach_goal(
        engine.current_state, goal, 
        available_force=0.5, 
        max_time=86400  # 1ì¼
    )
    print(f"   {'âœ…' if can_reach else 'âŒ'} {message}")
    
    # ê²°ì • ì‹œì‘
    print("\n" + "-" * 70)
    print("ğŸš€ Starting Decision: THROTTLE (magnitude=0.7)")
    
    action = ForceVector(ActionType.THROTTLE, magnitude=0.7)
    log = engine.start_decision(intention, action)
    
    print(f"   Decision ID: {log.decision_id}")
    print(f"   Predicted Î”: {log.predicted_delta.to_dict() if log.predicted_delta else 'N/A'}")
    
    # ì‹œë®¬ë ˆì´ì…˜: 3ì‹œê°„ í›„ ê²°ê³¼
    import random
    simulated_state = StateVector(
        energy=0.78 + random.uniform(-0.02, 0.02),
        flow=0.58 + random.uniform(-0.03, 0.03),
        risk=0.14 + random.uniform(-0.01, 0.01)
    )
    
    log = engine.complete_decision(log, simulated_state, delta_t=10800)  # 3ì‹œê°„
    
    print("\n" + "-" * 70)
    print("âœ… Decision Complete")
    print(f"\n{log.to_json()}")
    
    # í•™ìŠµ ìš”ì•½
    print("\n" + "-" * 70)
    print("ğŸ“Š Learning Summary")
    summary = engine.get_learning_summary()
    for k, v in summary.items():
        print(f"   {k}: {v:.4f}" if isinstance(v, float) else f"   {k}: {v}")
    
    print("\n" + "=" * 70)
    print("ğŸŒŒ Life Physics: Every decision is a force vector in state space.")
    print("=" * 70)
