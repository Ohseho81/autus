#!/usr/bin/env python3
"""
AUTUS 2.0 Distiller Engine
==========================
Raw Dataì—ì„œ 7ëŒ€ ë…¸ì´ì¦ˆ ì§€í‘œë¥¼ ì¦ë¥˜(Distill)í•˜ì—¬ HUD JSON ë°˜í™˜

7ëŒ€ ë…¸ì´ì¦ˆ ì§€í‘œ:
1. BIAS       - ì„ ì…ê²¬: ê³¼ê±° íŒ¨í„´ì— ëŒ€í•œ ê³¼ë„í•œ ì˜ì¡´
2. SCARCITY   - ì¡°ì‚¬ë¶€ì¡±: ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±
3. STAGNATION - ì‹¤í–‰ì§€ì—°: ê²°ì • í›„ ì‹¤í–‰ê¹Œì§€ì˜ ì§€ì—°
4. ATTACHMENT - ê°ì •ë§¤ëª°: ë¹„í•©ë¦¬ì  ê°ì •ì  ì§‘ì°©
5. FRICTION   - ìì›ê°„ì„­: ìì› ë°°ë¶„ì˜ ë¹„íš¨ìœ¨ì„±
6. HORIZON    - ë§¥ë½ê·¼ì‹œ: ë‹¨ê¸° ì‹œì•¼ë¡œ ì¸í•œ ì¥ê¸° ì†ì‹¤
7. PARADOX    - ì •ë³´ë§ˆë¹„: ê³¼ì‰ ì •ë³´ë¡œ ì¸í•œ ê²°ì • ë¶ˆëŠ¥

Usage:
    python3 autus_distiller.py --input "ë²•ì¸ ë¶€ì±„ 5ì–µ ìƒí™˜ vs ì‹ ê·œ ì‚¬ì—… 3ì–µ íˆ¬ì…"
"""

import os
import sys
import json
import shutil
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from typing import List, Dict, Optional, Tuple
from enum import Enum
import re
import math

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7ëŒ€ ë…¸ì´ì¦ˆ ì§€í‘œ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class NoiseType(Enum):
    BIAS = "BIAS"               # ì„ ì…ê²¬
    SCARCITY = "SCARCITY"       # ì¡°ì‚¬ë¶€ì¡±
    STAGNATION = "STAGNATION"   # ì‹¤í–‰ì§€ì—°
    ATTACHMENT = "ATTACHMENT"   # ê°ì •ë§¤ëª°
    FRICTION = "FRICTION"       # ìì›ê°„ì„­
    HORIZON = "HORIZON"         # ë§¥ë½ê·¼ì‹œ
    PARADOX = "PARADOX"         # ì •ë³´ë§ˆë¹„

NOISE_THRESHOLDS = {
    NoiseType.BIAS: 0.7,
    NoiseType.SCARCITY: 0.6,
    NoiseType.STAGNATION: 0.5,  # 72ì‹œê°„ ê¸°ì¤€ ì •ê·œí™”
    NoiseType.ATTACHMENT: 0.5,
    NoiseType.FRICTION: 0.4,
    NoiseType.HORIZON: 0.6,
    NoiseType.PARADOX: 0.5,
}

NOISE_KOREAN = {
    NoiseType.BIAS: "ì„ ì…ê²¬",
    NoiseType.SCARCITY: "ì¡°ì‚¬ë¶€ì¡±",
    NoiseType.STAGNATION: "ì‹¤í–‰ì§€ì—°",
    NoiseType.ATTACHMENT: "ê°ì •ë§¤ëª°",
    NoiseType.FRICTION: "ìì›ê°„ì„­",
    NoiseType.HORIZON: "ë§¥ë½ê·¼ì‹œ",
    NoiseType.PARADOX: "ì •ë³´ë§ˆë¹„",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class NoiseIndicator:
    """ë‹¨ì¼ ë…¸ì´ì¦ˆ ì§€í‘œ"""
    type: str
    name_kr: str
    score: float           # 0.0 ~ 1.0
    threshold: float
    status: str            # SAFE, WARNING, DANGER
    evidence: str          # ê·¼ê±°
    impact_won: float      # ì˜ˆìƒ ì†ì‹¤ (ì›)
    
@dataclass
class HUDOutput:
    """HUD ìŠ¤íƒ€ì¼ ì¶œë ¥ ë°ì´í„°"""
    timestamp: str
    input_hash: str
    
    # í•µì‹¬ ì§€í‘œ
    loss_velocity: float           # ì†ì‹¤ ì†ë„ (ì›/ì´ˆ)
    pnr_days: int                  # Point of No Returnê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜
    mva: str                       # Minimal Viable Action
    
    # 7ëŒ€ ë…¸ì´ì¦ˆ
    noise_indicators: List[NoiseIndicator]
    dominant_noise: str            # ê°€ì¥ ë†’ì€ ë…¸ì´ì¦ˆ
    total_noise_score: float       # ì¢…í•© ë…¸ì´ì¦ˆ ì ìˆ˜
    
    # ì˜ì‚¬ê²°ì • ì§€ì›
    recommended_action: str
    alternative_paths: List[str]
    risk_assessment: str
    
    # ë©”íƒ€ë°ì´í„°
    vault_path: Optional[str] = None
    model_used: str = "distiller-v2.0"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DISTILLER ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Distiller:
    """7ëŒ€ ë…¸ì´ì¦ˆ ì¦ë¥˜ê¸°"""
    
    # ê°ì •/í¸í–¥ í‚¤ì›Œë“œ
    BIAS_KEYWORDS = ["í•­ìƒ", "ëŠ˜", "ë‹¹ì—°íˆ", "ì›ë˜", "ì˜ˆì „ë¶€í„°", "ì „í†µì ìœ¼ë¡œ", "ê´€ë¡€ìƒ"]
    ATTACHMENT_KEYWORDS = ["ì ˆëŒ€", "ë°˜ë“œì‹œ", "ê¼­", "ë¬´ì¡°ê±´", "í¬ê¸°í•  ìˆ˜ ì—†", "ì• ì°©", "ì •ë“¤"]
    HORIZON_KEYWORDS = ["ë‹¹ì¥", "ì§€ê¸ˆ", "ê¸‰í•´", "ì¼ë‹¨", "ë‚˜ì¤‘ì—", "ì–¸ì  ê°€"]
    PARADOX_KEYWORDS = ["ê³ ë¯¼", "ê°ˆë“±", "ì„ íƒ", "vs", "ì•„ë‹ˆë©´", "ë˜ëŠ”", "í•œí¸"]
    
    def __init__(self, vault_path: str = "./vault"):
        self.vault_path = vault_path
        os.makedirs(vault_path, exist_ok=True)
    
    def distill(self, raw_input: str, context: Dict = None) -> HUDOutput:
        """
        Raw Inputì—ì„œ 7ëŒ€ ë…¸ì´ì¦ˆ ì§€í‘œ ì¦ë¥˜
        
        Args:
            raw_input: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ì˜ì‚¬ê²°ì • ìƒí™©)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì¬ë¬´ ë°ì´í„° ë“±)
        
        Returns:
            HUDOutput: HUD ìŠ¤íƒ€ì¼ ë¶„ì„ ê²°ê³¼
        """
        context = context or {}
        timestamp = datetime.now().isoformat()
        input_hash = hashlib.md5(raw_input.encode()).hexdigest()[:8]
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        amounts = self._extract_amounts(raw_input)
        total_amount = sum(amounts) if amounts else 100_000_000  # ê¸°ë³¸ 1ì–µ
        
        # 7ëŒ€ ë…¸ì´ì¦ˆ ê³„ì‚°
        indicators = []
        
        # 1. BIAS (ì„ ì…ê²¬)
        bias_score = self._calculate_bias(raw_input)
        indicators.append(self._create_indicator(
            NoiseType.BIAS, bias_score,
            "ê³¼ê±° íŒ¨í„´/ê´€ë¡€ì— ì˜ì¡´í•˜ëŠ” í‘œí˜„ ê°ì§€" if bias_score > 0.3 else "ê°ê´€ì  ë¶„ì„ ê¸°ë°˜",
            total_amount * bias_score * 0.15
        ))
        
        # 2. SCARCITY (ì¡°ì‚¬ë¶€ì¡±)
        scarcity_score = self._calculate_scarcity(raw_input, context)
        indicators.append(self._create_indicator(
            NoiseType.SCARCITY, scarcity_score,
            "ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ ë°ì´í„° ë¶€ì¡±" if scarcity_score > 0.4 else "ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´",
            total_amount * scarcity_score * 0.2
        ))
        
        # 3. STAGNATION (ì‹¤í–‰ì§€ì—°)
        stagnation_score = self._calculate_stagnation(raw_input, context)
        indicators.append(self._create_indicator(
            NoiseType.STAGNATION, stagnation_score,
            "ê²°ì •-ì‹¤í–‰ ê°„ ì§€ì—° ë¦¬ìŠ¤í¬" if stagnation_score > 0.3 else "ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ",
            total_amount * stagnation_score * 0.1
        ))
        
        # 4. ATTACHMENT (ê°ì •ë§¤ëª°)
        attachment_score = self._calculate_attachment(raw_input)
        indicators.append(self._create_indicator(
            NoiseType.ATTACHMENT, attachment_score,
            "ê°ì •ì  ì§‘ì°© í‘œí˜„ ê°ì§€" if attachment_score > 0.3 else "í•©ë¦¬ì  íŒë‹¨ ê°€ëŠ¥",
            total_amount * attachment_score * 0.25
        ))
        
        # 5. FRICTION (ìì›ê°„ì„­)
        friction_score = self._calculate_friction(raw_input, amounts)
        indicators.append(self._create_indicator(
            NoiseType.FRICTION, friction_score,
            "ìì› ë°°ë¶„ ì¶©ëŒ ê°ì§€" if friction_score > 0.3 else "ìì› ë°°ë¶„ ìµœì í™”ë¨",
            total_amount * friction_score * 0.15
        ))
        
        # 6. HORIZON (ë§¥ë½ê·¼ì‹œ)
        horizon_score = self._calculate_horizon(raw_input)
        indicators.append(self._create_indicator(
            NoiseType.HORIZON, horizon_score,
            "ë‹¨ê¸° ì‹œì•¼ í¸í–¥ ê°ì§€" if horizon_score > 0.4 else "ì¥ê¸° ê´€ì  ìœ ì§€",
            total_amount * horizon_score * 0.2
        ))
        
        # 7. PARADOX (ì •ë³´ë§ˆë¹„)
        paradox_score = self._calculate_paradox(raw_input)
        indicators.append(self._create_indicator(
            NoiseType.PARADOX, paradox_score,
            "ì„ íƒì§€ ê³¼ì‰ìœ¼ë¡œ ê²°ì • ì§€ì—°" if paradox_score > 0.4 else "ëª…í™•í•œ ì„ íƒì§€ ì¡´ì¬",
            total_amount * paradox_score * 0.1
        ))
        
        # ì¢…í•© ë¶„ì„
        total_noise = sum(ind.score for ind in indicators) / len(indicators)
        dominant = max(indicators, key=lambda x: x.score)
        total_impact = sum(ind.impact_won for ind in indicators)
        
        # ì†ì‹¤ ì†ë„ ê³„ì‚° (ì›”ê°„ ì†ì‹¤ â†’ ì´ˆë‹¹ ì†ì‹¤)
        loss_velocity = total_impact / (30 * 24 * 60 * 60)
        
        # PNR ê³„ì‚° (ìì› ì†Œì§„ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜)
        pnr_days = self._calculate_pnr(total_amount, total_impact)
        
        # MVA ìƒì„±
        mva = self._generate_mva(dominant, raw_input, amounts)
        
        # ëŒ€ì•ˆ ê²½ë¡œ
        alternatives = self._generate_alternatives(raw_input, indicators)
        
        # ë¦¬ìŠ¤í¬ í‰ê°€
        risk = "HIGH" if total_noise > 0.6 else ("MEDIUM" if total_noise > 0.4 else "LOW")
        
        # Raw Dataë¥¼ Vaultë¡œ ì´ë™
        vault_file = self._archive_to_vault(raw_input, input_hash, timestamp)
        
        return HUDOutput(
            timestamp=timestamp,
            input_hash=input_hash,
            loss_velocity=round(loss_velocity, 2),
            pnr_days=pnr_days,
            mva=mva,
            noise_indicators=indicators,
            dominant_noise=dominant.type,
            total_noise_score=round(total_noise, 3),
            recommended_action=mva,
            alternative_paths=alternatives,
            risk_assessment=risk,
            vault_path=vault_file
        )
    
    def _extract_amounts(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡ ì¶”ì¶œ"""
        amounts = []
        patterns = [
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*ì–µ',
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*ë§Œ',
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*ì›',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                num = float(match.replace(',', ''))
                if 'ì–µ' in text[text.find(match):text.find(match)+10]:
                    amounts.append(num * 100_000_000)
                elif 'ë§Œ' in text[text.find(match):text.find(match)+10]:
                    amounts.append(num * 10_000)
                else:
                    amounts.append(num)
        
        return amounts if amounts else [100_000_000]
    
    def _calculate_bias(self, text: str) -> float:
        """ì„ ì…ê²¬ ì ìˆ˜ ê³„ì‚°"""
        count = sum(1 for kw in self.BIAS_KEYWORDS if kw in text)
        base_score = min(count * 0.15, 0.6)
        
        # ìˆ«ì/ë°ì´í„° ì–¸ê¸‰ì´ ì ìœ¼ë©´ í¸í–¥ ì¦ê°€
        numbers = len(re.findall(r'\d+', text))
        if numbers < 2:
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _calculate_scarcity(self, text: str, context: Dict) -> float:
        """ì¡°ì‚¬ë¶€ì¡± ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ê°’
        
        # ìˆ«ìê°€ ë§ìœ¼ë©´ ì¡°ì‚¬ê°€ ëœ ê²ƒ
        numbers = len(re.findall(r'\d+', text))
        score -= numbers * 0.05
        
        # ì»¨í…ìŠ¤íŠ¸ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°ì†Œ
        if context.get('financial_data'):
            score -= 0.2
        if context.get('market_data'):
            score -= 0.15
        
        # 'ëª¨ë¥´' 'ë¶ˆí™•ì‹¤' 'ì˜ˆìƒ' ë“±ì´ ìˆìœ¼ë©´ ì¦ê°€
        uncertainty = ['ëª¨ë¥´', 'ë¶ˆí™•ì‹¤', 'ì˜ˆìƒ', 'ì¶”ì •', 'ì•„ë§ˆ', 'ê²ƒ ê°™']
        score += sum(0.1 for kw in uncertainty if kw in text)
        
        return max(0, min(score, 1.0))
    
    def _calculate_stagnation(self, text: str, context: Dict) -> float:
        """ì‹¤í–‰ì§€ì—° ì ìˆ˜ ê³„ì‚°"""
        score = 0.3
        
        # ì§€ì—° í‚¤ì›Œë“œ
        delay = ['ë‚˜ì¤‘ì—', 'ê²€í† ', 'ê³ ë ¤', 'ìƒê°í•´', 'ë¯¸ë£¨', 'ë³´ë¥˜', 'ëŒ€ê¸°']
        score += sum(0.1 for kw in delay if kw in text)
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ ê²°ì • ì‹œì  í™•ì¸
        if context.get('last_decision_days', 0) > 30:
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_attachment(self, text: str) -> float:
        """ê°ì •ë§¤ëª° ì ìˆ˜ ê³„ì‚°"""
        count = sum(1 for kw in self.ATTACHMENT_KEYWORDS if kw in text)
        base_score = min(count * 0.2, 0.6)
        
        # ê°ì • í‘œí˜„
        emotions = ['ì‹«', 'ì¢‹', 'ì›í•´', 'ë°”ë¼', 'í¬ë§', 'ë‘ë ¤', 'ê±±ì •']
        base_score += sum(0.08 for kw in emotions if kw in text)
        
        return min(base_score, 1.0)
    
    def _calculate_friction(self, text: str, amounts: List[float]) -> float:
        """ìì›ê°„ì„­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.2
        
        # ê¸ˆì•¡ì´ ì—¬ëŸ¬ ê°œë©´ ìì› ì¶©ëŒ ê°€ëŠ¥ì„±
        if len(amounts) > 1:
            score += 0.15 * (len(amounts) - 1)
        
        # vs, ëŒ€ì‹ , ë˜ëŠ” ë“± ëŒ€ë¦½ í‘œí˜„
        conflict = ['vs', 'ëŒ€ì‹ ', 'ë˜ëŠ”', 'ì•„ë‹ˆë©´', 'ëŒ€ë¹„', 'ë¹„êµ']
        score += sum(0.1 for kw in conflict if kw in text.lower())
        
        return min(score, 1.0)
    
    def _calculate_horizon(self, text: str) -> float:
        """ë§¥ë½ê·¼ì‹œ ì ìˆ˜ ê³„ì‚°"""
        count = sum(1 for kw in self.HORIZON_KEYWORDS if kw in text)
        base_score = min(count * 0.15, 0.5)
        
        # ì¥ê¸° í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°ì†Œ
        long_term = ['ì¥ê¸°', 'ë¯¸ë˜', '5ë…„', '10ë…„', 'ì „ëµì ', 'ì§€ì†']
        base_score -= sum(0.1 for kw in long_term if kw in text)
        
        return max(0, min(base_score, 1.0))
    
    def _calculate_paradox(self, text: str) -> float:
        """ì •ë³´ë§ˆë¹„ ì ìˆ˜ ê³„ì‚°"""
        count = sum(1 for kw in self.PARADOX_KEYWORDS if kw in text)
        base_score = min(count * 0.15, 0.5)
        
        # ì„ íƒì§€ê°€ ë§ìœ¼ë©´ ì¦ê°€
        options = text.count(',') + text.count('ë˜ëŠ”') + text.count('vs')
        base_score += options * 0.05
        
        return min(base_score, 1.0)
    
    def _create_indicator(self, noise_type: NoiseType, score: float, 
                          evidence: str, impact: float) -> NoiseIndicator:
        """ë…¸ì´ì¦ˆ ì§€í‘œ ê°ì²´ ìƒì„±"""
        threshold = NOISE_THRESHOLDS[noise_type]
        
        if score >= threshold:
            status = "DANGER"
        elif score >= threshold * 0.7:
            status = "WARNING"
        else:
            status = "SAFE"
        
        return NoiseIndicator(
            type=noise_type.value,
            name_kr=NOISE_KOREAN[noise_type],
            score=round(score, 3),
            threshold=threshold,
            status=status,
            evidence=evidence,
            impact_won=round(impact, 0)
        )
    
    def _calculate_pnr(self, total_amount: float, monthly_impact: float) -> int:
        """Point of No Return ê³„ì‚°"""
        if monthly_impact <= 0:
            return 365
        
        days = int((total_amount * 0.3) / (monthly_impact / 30))  # 30% ì†Œì§„ ê¸°ì¤€
        return max(1, min(days, 365))
    
    def _generate_mva(self, dominant: NoiseIndicator, text: str, 
                      amounts: List[float]) -> str:
        """Minimal Viable Action ìƒì„±"""
        mva_templates = {
            "BIAS": "ê³¼ê±° ë°ì´í„° 3ê±´ ì´ìƒ ìˆ˜ì§‘ í›„ ì¬ë¶„ì„",
            "SCARCITY": "í•µì‹¬ ì§€í‘œ 5ê°œ ì •ëŸ‰í™” í›„ ì¬ê²€í† ",
            "STAGNATION": "48ì‹œê°„ ë‚´ 1ì°¨ ì‹¤í–‰ ì°©ìˆ˜",
            "ATTACHMENT": "ì œ3ì ê°ê´€ì  ë¦¬ë·° ìš”ì²­",
            "FRICTION": "ìì› ë°°ë¶„ ìš°ì„ ìˆœìœ„ ì¬ì •ë ¬",
            "HORIZON": "5ë…„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰",
            "PARADOX": "ì„ íƒì§€ë¥¼ 2ê°œë¡œ ì¶•ì†Œ í›„ ê²°ì •",
        }
        
        base_mva = mva_templates.get(dominant.type, "ì¦‰ì‹œ ì‹¤í–‰")
        
        # ê¸ˆì•¡ ê¸°ë°˜ êµ¬ì²´í™”
        if amounts and amounts[0] >= 100_000_000:
            return f"{base_mva} (ê´€ë ¨ ê¸ˆì•¡: {amounts[0]/100_000_000:.1f}ì–µ)"
        
        return base_mva
    
    def _generate_alternatives(self, text: str, 
                               indicators: List[NoiseIndicator]) -> List[str]:
        """ëŒ€ì•ˆ ê²½ë¡œ ìƒì„±"""
        alternatives = []
        
        sorted_noise = sorted(indicators, key=lambda x: x.score, reverse=True)
        
        for ind in sorted_noise[:3]:
            if ind.status != "SAFE":
                if ind.type == "BIAS":
                    alternatives.append("ê²½ìŸì‚¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í™•ë³´")
                elif ind.type == "SCARCITY":
                    alternatives.append("ì™¸ë¶€ ì „ë¬¸ê°€ ìë¬¸ ì˜ë¢°")
                elif ind.type == "STAGNATION":
                    alternatives.append("íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ë¡œ ì†Œê·œëª¨ ì„ ì‹¤í–‰")
                elif ind.type == "ATTACHMENT":
                    alternatives.append("ì†ì ˆ ê¸°ì¤€ì  ì‚¬ì „ ì„¤ì •")
                elif ind.type == "FRICTION":
                    alternatives.append("ìì› í’€ ë¶„ë¦¬ ìš´ì˜")
                elif ind.type == "HORIZON":
                    alternatives.append("ì¥ê¸° ì‹œë‚˜ë¦¬ì˜¤ 3ê°œ ì‘ì„±")
                elif ind.type == "PARADOX":
                    alternatives.append("ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤ ì‘ì„±")
        
        return alternatives if alternatives else ["í˜„ì¬ ê³„íš ìœ ì§€"]
    
    def _archive_to_vault(self, raw_input: str, hash_id: str, 
                          timestamp: str) -> str:
        """Raw Dataë¥¼ Vaultë¡œ ì•„ì¹´ì´ë¸Œ"""
        date_str = timestamp.split("T")[0]
        filename = f"{date_str}_{hash_id}.json"
        filepath = os.path.join(self.vault_path, filename)
        
        archive_data = {
            "archived_at": timestamp,
            "hash": hash_id,
            "raw_input": raw_input,
            "status": "archived"
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(archive_data, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def to_hud_json(self, output: HUDOutput) -> str:
        """HUD ìŠ¤íƒ€ì¼ JSON ë³€í™˜"""
        
        def serialize(obj):
            if isinstance(obj, NoiseIndicator):
                return asdict(obj)
            return obj
        
        data = {
            "hud_version": "2.0",
            "timestamp": output.timestamp,
            "hash": output.input_hash,
            "core_metrics": {
                "loss_velocity_won_sec": output.loss_velocity,
                "pnr_days": output.pnr_days,
                "mva": output.mva,
                "risk": output.risk_assessment
            },
            "noise_analysis": {
                "dominant": output.dominant_noise,
                "total_score": output.total_noise_score,
                "indicators": [asdict(ind) for ind in output.noise_indicators]
            },
            "actions": {
                "recommended": output.recommended_action,
                "alternatives": output.alternative_paths
            },
            "vault": output.vault_path
        }
        
        return json.dumps(data, ensure_ascii=False, indent=2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="AUTUS 2.0 Distiller")
    parser.add_argument("--input", "-i", required=True, help="ë¶„ì„í•  ì˜ì‚¬ê²°ì • ìƒí™©")
    parser.add_argument("--output", "-o", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--json", action="store_true", help="JSON í˜•ì‹ ì¶œë ¥")
    
    args = parser.parse_args()
    
    distiller = Distiller()
    result = distiller.distill(args.input)
    
    if args.json:
        print(distiller.to_hud_json(result))
    else:
        # HUD ìŠ¤íƒ€ì¼ ì½˜ì†” ì¶œë ¥ì€ autus_hud.pyì—ì„œ ì²˜ë¦¬
        from autus_hud import HUDRenderer
        renderer = HUDRenderer()
        renderer.render(result)
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(distiller.to_hud_json(result))
        print(f"\nğŸ’¾ Saved to: {args.output}")


if __name__ == "__main__":
    main()
