"""
AUTUS OpenAI Behavior Drift ê°ì§€
================================

LLM ì¶œë ¥ ë³€í™” ê°ì§€ ì‹œìŠ¤í…œ

ë°©ë²•:
1. ìƒ˜í”Œ ì…ë ¥ 5~10ê°œì— ëŒ€í•´ ì¶œë ¥ ë¹„êµ
2. Cosine Similarity ì¸¡ì •
3. Perplexity ë³€í™”ìœ¨ ì¸¡ì •

ì„ê³„ê°’:
- cosine_sim < 0.92 â†’ human escalation
- Î”perplexity > +8% â†’ human escalation
"""

import logging
import hashlib
import json
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional
import os

logger = logging.getLogger(__name__)


@dataclass
class DriftTestCase:
    """Drift í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    input_text: str
    expected_output: str = ""
    baseline_output: str = ""
    new_output: str = ""
    cosine_similarity: float = 1.0
    perplexity_baseline: float = 0.0
    perplexity_new: float = 0.0


@dataclass
class DriftResult:
    """Drift ê°ì§€ ê²°ê³¼"""
    is_safe: bool = True
    avg_cosine_similarity: float = 1.0
    delta_perplexity_percent: float = 0.0
    test_cases_passed: int = 0
    test_cases_failed: int = 0
    escalation_reason: str = ""
    details: list = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)


# í‘œì¤€ í…ŒìŠ¤íŠ¸ ì…ë ¥ (AUTUS ë„ë©”ì¸ íŠ¹í™”)
STANDARD_TEST_INPUTS = [
    {
        "input": "HR ì˜¨ë³´ë”© í”„ë¡œì„¸ìŠ¤ë¥¼ ìµœì í™”í•˜ë ¤ë©´ ì–´ë–¤ ë‹¨ê³„ê°€ í•„ìš”í•œê°€ìš”?",
        "expected_keywords": ["ì˜¨ë³´ë”©", "í”„ë¡œì„¸ìŠ¤", "ë‹¨ê³„", "ìµœì í™”"],
    },
    {
        "input": "Inertia Debtê°€ 0.8ì„ ì´ˆê³¼í–ˆì„ ë•Œ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ë‚˜ìš”?",
        "expected_keywords": ["inertia", "debt", "ì¡°ì¹˜", "ìœ„í—˜"],
    },
    {
        "input": "1-12-144 ê´€ê³„ ê·¸ë˜í”„ì—ì„œ connectivity densityë¥¼ ë†’ì´ë ¤ë©´?",
        "expected_keywords": ["ê´€ê³„", "ê·¸ë˜í”„", "ì—°ê²°", "ë°€ë„"],
    },
    {
        "input": "Î”á¹ ê°€ ê¸‰ê²©íˆ ìƒìŠ¹í–ˆì„ ë•Œ Safety Guardì˜ ë™ì‘ì€?",
        "expected_keywords": ["safety", "guard", "ì—”íŠ¸ë¡œí”¼", "ìƒìŠ¹"],
    },
    {
        "input": "APQC PCF 7.4 ê¸°ì¤€ ì¬ë¬´ í”„ë¡œì„¸ìŠ¤ ëª¨ë“ˆì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
        "expected_keywords": ["APQC", "ì¬ë¬´", "í”„ë¡œì„¸ìŠ¤", "ëª¨ë“ˆ"],
    },
]


class BehaviorDriftDetector:
    """OpenAI Behavior Drift ê°ì§€ê¸°"""
    
    # ì„ê³„ê°’
    COSINE_SIM_THRESHOLD = 0.92
    PERPLEXITY_DELTA_THRESHOLD = 8.0  # %
    
    def __init__(self, model: str = "gpt-4o-mini"):
        """
        Args:
            model: í…ŒìŠ¤íŠ¸í•  OpenAI ëª¨ë¸
        """
        self.model = model
        self._client = None
        self._baseline_cache = {}
    
    def _get_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if self._client is None:
            try:
                from openai import OpenAI
                self._client = OpenAI()
            except ImportError:
                logger.warning("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            except Exception as e:
                logger.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return None
        return self._client
    
    def _get_embedding(self, text: str) -> list[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        client = self._get_client()
        if client is None:
            # í´ë°±: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„°
            return self._hash_to_vector(text)
        
        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
            )
            return response.data[0].embedding
        except Exception as e:
            logger.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return self._hash_to_vector(text)
    
    def _hash_to_vector(self, text: str, dim: int = 128) -> list[float]:
        """í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (í´ë°±)"""
        hash_bytes = hashlib.sha256(text.encode()).digest()
        vector = []
        for i in range(dim):
            byte_val = hash_bytes[i % len(hash_bytes)]
            vector.append((byte_val / 255.0) * 2 - 1)  # -1 ~ 1 ë²”ìœ„
        return vector
    
    def _cosine_similarity(self, vec1: list[float], vec2: list[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(vec1) != len(vec2):
            min_len = min(len(vec1), len(vec2))
            vec1 = vec1[:min_len]
            vec2 = vec2[:min_len]
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _estimate_perplexity(self, text: str) -> float:
        """Perplexity ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        # ì‹¤ì œë¡œëŠ” logprobs ì‚¬ìš©, ì—¬ê¸°ì„œëŠ” íœ´ë¦¬ìŠ¤í‹±
        words = text.split()
        unique_ratio = len(set(words)) / max(len(words), 1)
        avg_word_len = sum(len(w) for w in words) / max(len(words), 1)
        
        # ë‚®ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸
        return 10.0 * (1 - unique_ratio) + avg_word_len
    
    def _generate_output(self, input_text: str) -> str:
        """ëª¨ë¸ ì¶œë ¥ ìƒì„±"""
        client = self._get_client()
        if client is None:
            # í´ë°±: ì…ë ¥ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
            return f"[Simulated Response] {input_text[:100]}..."
        
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "AUTUS ì‹œìŠ¤í…œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                    {"role": "user", "content": input_text},
                ],
                max_tokens=200,
                temperature=0.1,  # ê²°ì •ë¡ ì  ì¶œë ¥ì„ ìœ„í•´ ë‚®ì€ temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"ì¶œë ¥ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"[Error] {e}"
    
    def get_baseline(self, test_cases: Optional[list] = None) -> dict:
        """
        ê¸°ì¤€ì„  ì¶œë ¥ ìƒì„± ë° ì €ì¥
        
        Args:
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ëª©ë¡ (Noneì´ë©´ ê¸°ë³¸ ì‚¬ìš©)
            
        Returns:
            dict: ì…ë ¥ í•´ì‹œ â†’ ì¶œë ¥ ë§¤í•‘
        """
        cases = test_cases or STANDARD_TEST_INPUTS
        baseline = {}
        
        logger.info("ğŸ“Š Baseline ì¶œë ¥ ìƒì„± ì¤‘...")
        
        for case in cases:
            input_text = case["input"]
            input_hash = hashlib.md5(input_text.encode()).hexdigest()[:8]
            
            output = self._generate_output(input_text)
            embedding = self._get_embedding(output)
            perplexity = self._estimate_perplexity(output)
            
            baseline[input_hash] = {
                "input": input_text,
                "output": output,
                "embedding": embedding,
                "perplexity": perplexity,
                "expected_keywords": case.get("expected_keywords", []),
            }
        
        self._baseline_cache = baseline
        return baseline
    
    def detect_drift(
        self,
        baseline: Optional[dict] = None,
        test_cases: Optional[list] = None,
    ) -> DriftResult:
        """
        Behavior Drift ê°ì§€
        
        Args:
            baseline: ê¸°ì¤€ì„  ë°ì´í„° (Noneì´ë©´ ìºì‹œ ì‚¬ìš©)
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            
        Returns:
            DriftResult: ê°ì§€ ê²°ê³¼
        """
        if baseline is None:
            baseline = self._baseline_cache or self.get_baseline(test_cases)
        
        cases = test_cases or STANDARD_TEST_INPUTS
        
        logger.info("ğŸ” Behavior Drift ê°ì§€ ì‹œì‘...")
        
        result = DriftResult()
        similarities = []
        perplexity_deltas = []
        
        for case in cases:
            input_text = case["input"]
            input_hash = hashlib.md5(input_text.encode()).hexdigest()[:8]
            
            if input_hash not in baseline:
                logger.warning(f"Baselineì— ì—†ëŠ” ì…ë ¥: {input_text[:30]}...")
                continue
            
            base_data = baseline[input_hash]
            
            # ìƒˆ ì¶œë ¥ ìƒì„±
            new_output = self._generate_output(input_text)
            new_embedding = self._get_embedding(new_output)
            new_perplexity = self._estimate_perplexity(new_output)
            
            # Cosine Similarity
            cosine_sim = self._cosine_similarity(base_data["embedding"], new_embedding)
            similarities.append(cosine_sim)
            
            # Perplexity Delta
            base_perplexity = base_data["perplexity"]
            if base_perplexity > 0:
                delta_ppl = ((new_perplexity - base_perplexity) / base_perplexity) * 100
            else:
                delta_ppl = 0.0
            perplexity_deltas.append(delta_ppl)
            
            # í‚¤ì›Œë“œ ì²´í¬
            expected_keywords = base_data.get("expected_keywords", [])
            keywords_found = sum(1 for kw in expected_keywords if kw.lower() in new_output.lower())
            keyword_ratio = keywords_found / max(len(expected_keywords), 1)
            
            # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
            case_passed = cosine_sim >= self.COSINE_SIM_THRESHOLD and delta_ppl < self.PERPLEXITY_DELTA_THRESHOLD
            
            if case_passed:
                result.test_cases_passed += 1
            else:
                result.test_cases_failed += 1
            
            result.details.append({
                "input_hash": input_hash,
                "cosine_similarity": round(cosine_sim, 4),
                "perplexity_delta_percent": round(delta_ppl, 2),
                "keyword_ratio": round(keyword_ratio, 2),
                "passed": case_passed,
            })
            
            status = "âœ…" if case_passed else "âŒ"
            logger.info(f"  {status} {input_hash}: cosine={cosine_sim:.3f}, Î”ppl={delta_ppl:.1f}%")
        
        # ì „ì²´ ê²°ê³¼ ê³„ì‚°
        if similarities:
            result.avg_cosine_similarity = sum(similarities) / len(similarities)
        if perplexity_deltas:
            result.delta_perplexity_percent = sum(perplexity_deltas) / len(perplexity_deltas)
        
        # ì•ˆì „ì„± íŒë‹¨
        escalation_reasons = []
        
        if result.avg_cosine_similarity < self.COSINE_SIM_THRESHOLD:
            escalation_reasons.append(
                f"Cosine Similarity ë‚®ìŒ: {result.avg_cosine_similarity:.3f} < {self.COSINE_SIM_THRESHOLD}"
            )
        
        if result.delta_perplexity_percent > self.PERPLEXITY_DELTA_THRESHOLD:
            escalation_reasons.append(
                f"Perplexity ì¦ê°€: +{result.delta_perplexity_percent:.1f}% > +{self.PERPLEXITY_DELTA_THRESHOLD}%"
            )
        
        if result.test_cases_failed > result.test_cases_passed:
            escalation_reasons.append(
                f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ê³¼ë‹¤: {result.test_cases_failed}/{result.test_cases_passed + result.test_cases_failed}"
            )
        
        result.is_safe = len(escalation_reasons) == 0
        result.escalation_reason = "; ".join(escalation_reasons) if escalation_reasons else "ëª¨ë“  ê²€ì‚¬ í†µê³¼"
        
        status = "âœ… ì•ˆì „" if result.is_safe else "ğŸš¨ Human Escalation í•„ìš”"
        logger.info(f"\nê²°ê³¼: {status}")
        logger.info(f"  í‰ê·  Cosine Sim: {result.avg_cosine_similarity:.4f}")
        logger.info(f"  í‰ê·  Î” Perplexity: {result.delta_perplexity_percent:.2f}%")
        
        return result


def run_drift_detection(model: str = "gpt-4o-mini") -> DriftResult:
    """
    Drift ê°ì§€ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜
    
    Args:
        model: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸
        
    Returns:
        DriftResult: ê°ì§€ ê²°ê³¼
    """
    detector = BehaviorDriftDetector(model=model)
    
    # ê¸°ì¤€ì„  ìƒì„±
    baseline = detector.get_baseline()
    
    # Drift ê°ì§€
    result = detector.detect_drift(baseline)
    
    return result
