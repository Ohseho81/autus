"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  AUTUS V Predictor v1.0 â€” AI ê¸°ë°˜ V ì˜ˆì¸¡
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

V ê³µì‹ì˜ AI í†µí•©:
- LSTM/GRU ì‹œê³„ì—´ ì˜ˆì¸¡
- ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ë³µë¦¬ ì„ í˜•í™”
- ì—°ì† í•™ìŠµ (Continual Learning)
- ì•™ìƒë¸” ì˜ˆì¸¡

ë¡œì»¬ (Zero-Cloud) í™˜ê²½ì—ì„œ ì‹¤í–‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
import math
import json
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì˜ì¡´ì„± ì²´í¬ (ë¡œì»¬ í™˜ê²½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NUMPY_AVAILABLE = False
TORCH_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    pass

try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# V ì‹œê³„ì—´ ë°ì´í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class VTimePoint:
    """V ì‹œê³„ì—´ ë°ì´í„° í¬ì¸íŠ¸"""
    timestamp: datetime
    M: float
    T: float
    s: float
    V: float
    network_density: float = 0.0
    decision_type: str = "accept"  # accept, reject, sync


@dataclass
class VHistory:
    """V íˆìŠ¤í† ë¦¬ ì»¬ë ‰ì…˜"""
    points: List[VTimePoint] = field(default_factory=list)
    
    def add(self, point: VTimePoint):
        self.points.append(point)
    
    def to_sequences(self, window_size: int = 7) -> List[List[float]]:
        """ì‹œê³„ì—´ì„ ìœˆë„ìš° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
        if len(self.points) < window_size:
            return []
        
        sequences = []
        for i in range(len(self.points) - window_size + 1):
            window = self.points[i:i + window_size]
            seq = [p.V for p in window]
            sequences.append(seq)
        
        return sequences
    
    def to_features(self) -> List[List[float]]:
        """íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜ [M, T, s, network_density]"""
        return [
            [p.M, p.T, p.s, p.network_density]
            for p in self.points
        ]
    
    def to_targets(self) -> List[float]:
        """íƒ€ê²Ÿ (Vê°’)"""
        return [p.V for p in self.points]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¡œê·¸ ë³€í™˜ ì„ í˜• ì˜ˆì¸¡ê¸° (Numpy ê¸°ë°˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LogLinearPredictor:
    """
    ë¡œê·¸ ë³€í™˜ ì„ í˜• ì˜ˆì¸¡ê¸°
    
    ë³µë¦¬ ê³µì‹ì˜ ë¡œê·¸ ë³€í™˜:
    log(V) = log(M-T) + t Ã— log(1+s)
    
    ì´ë¥¼ ì„ í˜• íšŒê·€ë¡œ í•™ìŠµ:
    y = a + bÃ—t  (ì—¬ê¸°ì„œ y = log(V))
    """
    
    def __init__(self):
        self.a = 0.0  # intercept
        self.b = 0.0  # slope (â‰ˆ log(1+s))
        self.trained = False
        self.r_squared = 0.0
    
    def fit(self, history: VHistory) -> Dict[str, float]:
        """
        íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¡œ í•™ìŠµ
        
        Returns:
            í•™ìŠµ ê²°ê³¼ (a, b, r_squared)
        """
        if not NUMPY_AVAILABLE:
            return self._fit_fallback(history)
        
        points = history.points
        if len(points) < 3:
            return {"error": "ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 3ê°œ í•„ìš”)"}
        
        # ì‹œê°„ ì¸ë±ìŠ¤ì™€ log(V) ì¶”ì¶œ
        t_values = np.array(range(len(points)))
        v_values = np.array([p.V for p in points])
        
        # Vê°€ 0 ì´í•˜ì¸ ê²½ìš° ì²˜ë¦¬
        v_values = np.clip(v_values, 0.01, None)
        log_v = np.log(v_values)
        
        # ì„ í˜• íšŒê·€ (ìµœì†Œì œê³±)
        n = len(t_values)
        sum_t = np.sum(t_values)
        sum_log_v = np.sum(log_v)
        sum_t_log_v = np.sum(t_values * log_v)
        sum_t_sq = np.sum(t_values ** 2)
        
        denominator = n * sum_t_sq - sum_t ** 2
        if denominator == 0:
            return {"error": "ê³„ì‚° ë¶ˆê°€ (ë¶„ëª¨ 0)"}
        
        self.b = (n * sum_t_log_v - sum_t * sum_log_v) / denominator
        self.a = (sum_log_v - self.b * sum_t) / n
        
        # RÂ² ê³„ì‚°
        predicted = self.a + self.b * t_values
        ss_res = np.sum((log_v - predicted) ** 2)
        ss_tot = np.sum((log_v - np.mean(log_v)) ** 2)
        self.r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        self.trained = True
        
        # ì¶”ì • synergy ê³„ì‚°: b â‰ˆ log(1+s) â†’ s â‰ˆ exp(b) - 1
        estimated_s = math.exp(self.b) - 1 if self.b > -1 else 0
        
        return {
            "a": round(self.a, 4),
            "b": round(self.b, 4),
            "r_squared": round(self.r_squared, 4),
            "estimated_s": round(estimated_s, 4),
            "data_points": len(points)
        }
    
    def _fit_fallback(self, history: VHistory) -> Dict[str, float]:
        """Numpy ì—†ì´ ìˆœìˆ˜ Pythonìœ¼ë¡œ í•™ìŠµ"""
        points = history.points
        if len(points) < 3:
            return {"error": "ë°ì´í„° ë¶€ì¡±"}
        
        t_values = list(range(len(points)))
        log_v = [math.log(max(0.01, p.V)) for p in points]
        
        n = len(t_values)
        sum_t = sum(t_values)
        sum_log_v = sum(log_v)
        sum_t_log_v = sum(t * lv for t, lv in zip(t_values, log_v))
        sum_t_sq = sum(t ** 2 for t in t_values)
        
        denominator = n * sum_t_sq - sum_t ** 2
        if denominator == 0:
            return {"error": "ê³„ì‚° ë¶ˆê°€"}
        
        self.b = (n * sum_t_log_v - sum_t * sum_log_v) / denominator
        self.a = (sum_log_v - self.b * sum_t) / n
        self.trained = True
        
        return {
            "a": round(self.a, 4),
            "b": round(self.b, 4),
            "estimated_s": round(math.exp(self.b) - 1, 4) if self.b > -1 else 0
        }
    
    def predict(self, future_t: int) -> Dict[str, float]:
        """ë¯¸ë˜ V ì˜ˆì¸¡"""
        if not self.trained:
            return {"error": "í•™ìŠµ í•„ìš”"}
        
        log_v_pred = self.a + self.b * future_t
        v_pred = math.exp(log_v_pred)
        
        return {
            "t": future_t,
            "predicted_V": round(v_pred, 2),
            "log_V": round(log_v_pred, 4),
            "confidence": self.r_squared
        }
    
    def predict_range(self, start_t: int, end_t: int) -> List[Dict[str, float]]:
        """ë²”ìœ„ ì˜ˆì¸¡"""
        return [self.predict(t) for t in range(start_t, end_t + 1)]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LSTM ì˜ˆì¸¡ê¸° (PyTorch ê¸°ë°˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if TORCH_AVAILABLE:
    class VLSTMModel(nn.Module):
        """V ì˜ˆì¸¡ìš© LSTM ëª¨ë¸"""
        
        def __init__(
            self,
            input_size: int = 4,    # [M, T, s, network_density]
            hidden_size: int = 32,
            num_layers: int = 2,
            output_size: int = 1    # V
        ):
            super().__init__()
            
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            
            self.lstm = nn.LSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=0.1 if num_layers > 1 else 0
            )
            
            self.fc = nn.Sequential(
                nn.Linear(hidden_size, 16),
                nn.ReLU(),
                nn.Linear(16, output_size)
            )
        
        def forward(self, x, hidden=None):
            # x: (batch, seq_len, input_size)
            lstm_out, hidden = self.lstm(x, hidden)
            # lstm_out: (batch, seq_len, hidden_size)
            
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥
            last_output = lstm_out[:, -1, :]
            
            # ìµœì¢… ì˜ˆì¸¡
            out = self.fc(last_output)
            return out, hidden


class LSTMPredictor:
    """
    LSTM ê¸°ë°˜ V ì˜ˆì¸¡ê¸°
    
    ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ:
    - ì…ë ¥: [M, T, s, network_density] ì‹œí€€ìŠ¤
    - ì¶œë ¥: ë¯¸ë˜ V
    """
    
    def __init__(self, window_size: int = 7):
        self.window_size = window_size
        self.model = None
        self.trained = False
        self.loss_history = []
        
        if TORCH_AVAILABLE:
            self.model = VLSTMModel()
    
    def fit(
        self,
        history: VHistory,
        epochs: int = 100,
        lr: float = 0.001
    ) -> Dict[str, any]:
        """LSTM í•™ìŠµ"""
        
        if not TORCH_AVAILABLE:
            return {"error": "PyTorch ë¯¸ì„¤ì¹˜"}
        
        if len(history.points) < self.window_size + 1:
            return {"error": f"ë°ì´í„° ë¶€ì¡± (ìµœì†Œ {self.window_size + 1}ê°œ í•„ìš”)"}
        
        # ë°ì´í„° ì¤€ë¹„
        features = history.to_features()
        targets = history.to_targets()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []
        for i in range(len(features) - self.window_size):
            X.append(features[i:i + self.window_size])
            y.append(targets[i + self.window_size])
        
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        
        # í•™ìŠµ
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        self.model.train()
        self.loss_history = []
        
        for epoch in range(epochs):
            optimizer.zero_grad()
            output, _ = self.model(X)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            
            self.loss_history.append(loss.item())
        
        self.trained = True
        
        return {
            "final_loss": round(self.loss_history[-1], 6),
            "epochs": epochs,
            "data_points": len(X)
        }
    
    def predict(self, recent_history: List[List[float]]) -> Dict[str, float]:
        """ë¯¸ë˜ V ì˜ˆì¸¡"""
        
        if not TORCH_AVAILABLE or not self.trained:
            return {"error": "í•™ìŠµ í•„ìš” ë˜ëŠ” PyTorch ë¯¸ì„¤ì¹˜"}
        
        if len(recent_history) < self.window_size:
            return {"error": f"ìµœê·¼ {self.window_size}ê°œ ë°ì´í„° í•„ìš”"}
        
        # ë§ˆì§€ë§‰ window_size ê°œ ì‚¬ìš©
        recent = recent_history[-self.window_size:]
        
        self.model.eval()
        with torch.no_grad():
            x = torch.tensor([recent], dtype=torch.float32)
            output, _ = self.model(x)
            predicted_v = output.item()
        
        return {
            "predicted_V": round(predicted_v, 2)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì•™ìƒë¸” ì˜ˆì¸¡ê¸°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EnsemblePredictor:
    """
    ì•™ìƒë¸” ì˜ˆì¸¡ê¸°
    
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©:
    - LogLinear (ë¹ ë¥´ê³  í•´ì„ ê°€ëŠ¥)
    - LSTM (ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
    
    ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
    """
    
    def __init__(self):
        self.log_linear = LogLinearPredictor()
        self.lstm = LSTMPredictor() if TORCH_AVAILABLE else None
        self.weights = {"log_linear": 0.5, "lstm": 0.5}
    
    def fit(self, history: VHistory) -> Dict[str, any]:
        """ì•™ìƒë¸” í•™ìŠµ"""
        
        results = {}
        
        # LogLinear í•™ìŠµ
        ll_result = self.log_linear.fit(history)
        results["log_linear"] = ll_result
        
        # LSTM í•™ìŠµ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.lstm:
            lstm_result = self.lstm.fit(history)
            results["lstm"] = lstm_result
        
        # ê°€ì¤‘ì¹˜ ì¡°ì • (RÂ² ê¸°ë°˜)
        if "r_squared" in ll_result and ll_result.get("r_squared", 0) > 0:
            r2 = ll_result["r_squared"]
            self.weights["log_linear"] = r2
            self.weights["lstm"] = 1 - r2
        
        results["weights"] = self.weights
        return results
    
    def predict(self, future_t: int, recent_features: List[List[float]] = None) -> Dict[str, any]:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        
        predictions = {}
        
        # LogLinear ì˜ˆì¸¡
        ll_pred = self.log_linear.predict(future_t)
        if "predicted_V" in ll_pred:
            predictions["log_linear"] = ll_pred["predicted_V"]
        
        # LSTM ì˜ˆì¸¡ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.lstm and self.lstm.trained and recent_features:
            lstm_pred = self.lstm.predict(recent_features)
            if "predicted_V" in lstm_pred:
                predictions["lstm"] = lstm_pred["predicted_V"]
        
        # ê°€ì¤‘ í‰ê· 
        if predictions:
            weighted_sum = 0
            total_weight = 0
            
            for model, v in predictions.items():
                weight = self.weights.get(model, 0.5)
                weighted_sum += v * weight
                total_weight += weight
            
            ensemble_v = weighted_sum / total_weight if total_weight > 0 else 0
            
            return {
                "ensemble_V": round(ensemble_v, 2),
                "individual": predictions,
                "weights": self.weights
            }
        
        return {"error": "ì˜ˆì¸¡ ë¶ˆê°€"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹±ê¸€í†¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_ensemble_instance: Optional[EnsemblePredictor] = None


def get_ensemble_predictor() -> EnsemblePredictor:
    """ì•™ìƒë¸” ì˜ˆì¸¡ê¸° ì‹±ê¸€í†¤"""
    global _ensemble_instance
    if _ensemble_instance is None:
        _ensemble_instance = EnsemblePredictor()
    return _ensemble_instance


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í¸ì˜ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_predictor(v_history: List[Dict]) -> Dict:
    """
    íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¡œ ì˜ˆì¸¡ê¸° í•™ìŠµ
    
    Args:
        v_history: [{"M": 100, "T": 40, "s": 0.3, "V": 60, "network_density": 0.1}, ...]
    
    Returns:
        í•™ìŠµ ê²°ê³¼
    """
    history = VHistory()
    
    for i, point in enumerate(v_history):
        history.add(VTimePoint(
            timestamp=datetime.now(),
            M=point.get("M", 0),
            T=point.get("T", 0),
            s=point.get("s", 0),
            V=point.get("V", 0),
            network_density=point.get("network_density", 0)
        ))
    
    predictor = get_ensemble_predictor()
    return predictor.fit(history)


def predict_future_v(future_months: int, recent_data: List[Dict] = None) -> Dict:
    """
    ë¯¸ë˜ V ì˜ˆì¸¡
    
    Args:
        future_months: ì˜ˆì¸¡í•  ë¯¸ë˜ ê¸°ê°„ (ì›”)
        recent_data: ìµœê·¼ ë°ì´í„° (LSTMìš©)
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼
    """
    predictor = get_ensemble_predictor()
    
    recent_features = None
    if recent_data:
        recent_features = [
            [d.get("M", 0), d.get("T", 0), d.get("s", 0), d.get("network_density", 0)]
            for d in recent_data
        ]
    
    return predictor.predict(future_months, recent_features)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("â•" * 60)
    print("  AUTUS V Predictor Test")
    print("â•" * 60)
    print(f"  NumPy: {'âœ…' if NUMPY_AVAILABLE else 'âŒ'}")
    print(f"  PyTorch: {'âœ…' if TORCH_AVAILABLE else 'âŒ'}")
    print("â”€" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ë³µë¦¬ ì„±ì¥ ì‹œë®¬)
    test_history = []
    base_v = 60
    s = 0.3
    for t in range(20):
        v = base_v * ((1 + s) ** t)
        test_history.append({
            "M": 100 + t * 5,
            "T": 40 + t * 2,
            "s": s,
            "V": v,
            "network_density": min(1, 0.1 + t * 0.05)
        })
    
    # í•™ìŠµ
    train_result = train_predictor(test_history)
    print("\ní•™ìŠµ ê²°ê³¼:")
    print(f"  LogLinear RÂ²: {train_result.get('log_linear', {}).get('r_squared', 'N/A')}")
    print(f"  ì¶”ì • Synergy: {train_result.get('log_linear', {}).get('estimated_s', 'N/A')}")
    
    # ì˜ˆì¸¡
    pred = predict_future_v(24, test_history[-7:])
    print(f"\n24ê°œì›” í›„ ì˜ˆì¸¡:")
    print(f"  Ensemble V: {pred.get('ensemble_V', 'N/A')}")
