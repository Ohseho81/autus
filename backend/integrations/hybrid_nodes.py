"""
AUTUS í•˜ì´ë¸Œë¦¬ë“œ LangGraph ë…¸ë“œ
==============================

TypeDB + Pinecone + DeepSeek-R1/Llama 3.3 í†µí•©

ì•„í‚¤í…ì²˜:
```
ì‚¬ìš©ì ëª…ë ¹ â†’ LangGraph Orchestration
              â†“
Analyzer â†’ TypeDB (ë³µì¡ ê´€ê³„Â·ìƒìˆ˜Â·ê³„ìˆ˜ ì¿¼ë¦¬)
              â†“
Retrieval â†’ Pinecone (ë²¡í„° ê²€ìƒ‰: ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸Â·ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì„ë² ë”©)
              â†“
Checker/Safety Guard â†’ TypeDB symbolic inference + Pinecone cosine sim
              â†“
Updater â†’ Canary (Vercel) + TypeDB ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
              â†“
Reporter â†’ TypeDB ê·œì¹™ ê¸°ë°˜ ìƒìˆ˜ ì¬ê³„ì‚° + Pinecone freshness í™•ì¸
```
"""

import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, Any, TypedDict
from enum import Enum

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•˜ì´ë¸Œë¦¬ë“œ ìƒíƒœ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HybridState(TypedDict, total=False):
    """í•˜ì´ë¸Œë¦¬ë“œ LangGraph ìƒíƒœ"""
    # ê¸°ë³¸
    user_id: str
    goal: str
    messages: list[str]
    
    # TypeDB ê²°ê³¼
    typedb_coefficients: dict
    typedb_breaking_techs: list[str]
    typedb_risk_inference: dict
    
    # Pinecone ê²°ê³¼
    pinecone_matches: list[dict]
    pinecone_similarity: float
    
    # LLM ê²°ê³¼
    llm_analysis: str
    llm_prediction: dict
    
    # Safety
    safety_route: str
    safety_details: dict
    
    # ìµœì¢…
    final_report: str
    success: bool


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•˜ì´ë¸Œë¦¬ë“œ ë…¸ë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def typedb_coefficient_node(state: HybridState) -> dict:
    """
    TypeDB ê³„ìˆ˜ ì¡°íšŒ ë…¸ë“œ
    
    1-12-144 ê·¸ë˜í”„ì—ì„œ ì‚¬ìš©ì ê³„ìˆ˜ ê³„ì‚°
    """
    from .typedb_client import TypeDBClient
    
    logger.info("ğŸ“Š TypeDB ê³„ìˆ˜ ì¡°íšŒ ë…¸ë“œ ì‹¤í–‰")
    
    client = TypeDBClient()
    client.connect()
    
    user_id = state.get("user_id", "user_ohseho_001")
    coefficients = client.query_user_coefficients(user_id)
    
    # ê³ ìœ„í—˜ ê¸°ìˆ  ì¡°íšŒ
    high_risk_techs = client.query_high_risk_technologies()
    breaking_techs = [t["tech_name"] for t in high_risk_techs]
    
    # Inertia Debt Rolling Average
    inertia_avg = client.query_inertia_debt_rolling_average(user_id)
    
    client.close()
    
    return {
        "typedb_coefficients": {
            **coefficients,
            "inertia_debt_avg": inertia_avg,
        },
        "typedb_breaking_techs": breaking_techs,
        "messages": state.get("messages", []) + ["TypeDB ê³„ìˆ˜ ì¡°íšŒ ì™„ë£Œ"],
    }


def pinecone_retrieval_node(state: HybridState) -> dict:
    """
    Pinecone ë²¡í„° ê²€ìƒ‰ ë…¸ë“œ
    
    ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸, ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰
    """
    from .pinecone_client import PineconeClient
    
    logger.info("ğŸ” Pinecone ë²¡í„° ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰")
    
    client = PineconeClient()
    client.connect()
    
    goal = state.get("goal", "")
    
    # ëª©í‘œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ì‹¤ì œë¡œëŠ” OpenAI/Claude ì„ë² ë”©)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„° ì‚¬ìš©
    query_embedding = _generate_mock_embedding(goal)
    
    # ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸ ê²€ìƒ‰
    matches = client.search_release_notes(
        query_embedding=query_embedding,
        top_k=5,
    )
    
    # Behavior Drift ì²´í¬ (ì´ì „ ê¸°ì¤€ì„ ê³¼ ë¹„êµ)
    drift_result = client.check_behavior_drift(
        model="gpt-4o-mini",
        input_hash=_hash_text(goal)[:8],
        new_embedding=query_embedding,
    )
    
    return {
        "pinecone_matches": matches,
        "pinecone_similarity": drift_result.get("similarity", 1.0),
        "messages": state.get("messages", []) + ["Pinecone ê²€ìƒ‰ ì™„ë£Œ"],
    }


def llm_analysis_node(state: HybridState) -> dict:
    """
    LLM ë¶„ì„ ë…¸ë“œ
    
    DeepSeek-R1 (reasoning) ë˜ëŠ” Llama 3.3 (instruction) ì„ íƒ
    """
    from .llm_selector import LLMSelector, TaskType
    
    logger.info("ğŸ¤– LLM ë¶„ì„ ë…¸ë“œ ì‹¤í–‰")
    
    selector = LLMSelector()
    
    # TypeDB ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    coefficients = state.get("typedb_coefficients", {})
    breaking_techs = state.get("typedb_breaking_techs", [])
    pinecone_matches = state.get("pinecone_matches", [])
    
    prompt = f"""
AUTUS ë¶„ì„ ìš”ì²­:

ëª©í‘œ: {state.get("goal", "")}

ì‚¬ìš©ì ê³„ìˆ˜:
- ì—°ê²° ë°€ë„: {coefficients.get("connectivity_density", 0):.2%}
- ì˜í–¥ë ¥: {coefficients.get("influence_score", 0):.2%}
- Inertia Debt í‰ê· : {coefficients.get("inertia_debt_avg", 0):.3f}

ê³ ìœ„í—˜ ê¸°ìˆ : {", ".join(breaking_techs) if breaking_techs else "ì—†ìŒ"}

ê´€ë ¨ ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸: {len(pinecone_matches)}ê°œ ë°œê²¬

ë‹¤ìŒì„ ë¶„ì„í•˜ì„¸ìš”:
1. ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥ì„± (0-100%)
2. ì£¼ìš” ìœ„í—˜ ìš”ì†Œ
3. ê¶Œì¥ ì¡°ì¹˜

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
"""
    
    # Reasoning íƒœìŠ¤í¬ì´ë¯€ë¡œ DeepSeek-R1 ì„ íƒ
    response = selector.generate(
        prompt=prompt,
        task_type=TaskType.REASONING,
        temperature=0.3,
    )
    
    # ì˜ˆì¸¡ íŒŒì‹± ì‹œë„
    prediction = _parse_json_response(response.content)
    
    return {
        "llm_analysis": response.content,
        "llm_prediction": prediction,
        "messages": state.get("messages", []) + [
            f"LLM ë¶„ì„ ì™„ë£Œ ({response.provider.value}/{response.model})"
        ],
    }


def hybrid_safety_guard_node(state: HybridState) -> dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ Safety Guard ë…¸ë“œ
    
    TypeDB inference + Pinecone drift + LLM prediction ì¢…í•©
    """
    logger.info("ğŸ›¡ï¸ í•˜ì´ë¸Œë¦¬ë“œ Safety Guard ë…¸ë“œ ì‹¤í–‰")
    
    # ì…ë ¥ ìˆ˜ì§‘
    coefficients = state.get("typedb_coefficients", {})
    pinecone_sim = state.get("pinecone_similarity", 1.0)
    prediction = state.get("llm_prediction", {})
    breaking_techs = state.get("typedb_breaking_techs", [])
    
    # ìœ„í—˜ ìš”ì†Œ í‰ê°€
    risks = []
    
    # 1. Inertia Debt ì²´í¬
    inertia_avg = coefficients.get("inertia_debt_avg", 0)
    if inertia_avg > 0.7:
        risks.append(f"Inertia Debt ë†’ìŒ: {inertia_avg:.3f}")
    
    # 2. Behavior Drift ì²´í¬
    if pinecone_sim < 0.92:
        risks.append(f"Behavior Drift ê°ì§€: sim={pinecone_sim:.3f}")
    
    # 3. Breaking Changes ì²´í¬
    if breaking_techs:
        risks.append(f"Breaking Change ê¸°ìˆ : {', '.join(breaking_techs)}")
    
    # 4. LLM ì˜ˆì¸¡ ìœ„í—˜ë„ ì²´í¬
    if prediction.get("risk_level") in ["HIGH", "CRITICAL"]:
        risks.append(f"LLM ìœ„í—˜ ì˜ˆì¸¡: {prediction.get('risk_level')}")
    
    # ë¼ìš°íŒ… ê²°ì •
    if len(risks) >= 3:
        route = "human_escalation"
    elif len(risks) >= 2:
        route = "throttle"
    elif len(risks) == 1:
        route = "continue_with_caution"
    else:
        route = "continue"
    
    return {
        "safety_route": route,
        "safety_details": {
            "risks": risks,
            "risk_count": len(risks),
            "inertia_debt_avg": inertia_avg,
            "pinecone_similarity": pinecone_sim,
            "breaking_techs_count": len(breaking_techs),
        },
        "messages": state.get("messages", []) + [f"Safety Guard: {route}"],
    }


def hybrid_report_node(state: HybridState) -> dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¦¬í¬íŠ¸ ë…¸ë“œ
    
    Llama 3.3ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    """
    from .llm_selector import LLMSelector, TaskType
    
    logger.info("ğŸ“ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬í¬íŠ¸ ë…¸ë“œ ì‹¤í–‰")
    
    selector = LLMSelector()
    
    # ë³´ê³ ì„œ í”„ë¡¬í”„íŠ¸
    prompt = f"""
AUTUS ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ëª©í‘œ: {state.get("goal", "")}

Safety Guard ê²°ê³¼: {state.get("safety_route", "")}
ìœ„í—˜ ìš”ì†Œ: {state.get("safety_details", {}).get("risks", [])}

LLM ë¶„ì„: {state.get("llm_analysis", "")[:500]}

í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. í¬í•¨ í•­ëª©:
1. ìš”ì•½ (2-3ë¬¸ì¥)
2. ì£¼ìš” ë°œê²¬ ì‚¬í•­
3. ê¶Œì¥ ì¡°ì¹˜
4. ë‹¤ìŒ ë‹¨ê³„
"""
    
    # Summarization íƒœìŠ¤í¬ì´ë¯€ë¡œ Llama 3.3 ì„ íƒ
    response = selector.generate(
        prompt=prompt,
        task_type=TaskType.SUMMARIZATION,
        temperature=0.5,
    )
    
    return {
        "final_report": response.content,
        "success": state.get("safety_route") in ["continue", "continue_with_caution"],
        "messages": state.get("messages", []) + ["ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ"],
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•˜ì´ë¸Œë¦¬ë“œ ê·¸ë˜í”„ ë¹Œë”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_hybrid_graph():
    """
    í•˜ì´ë¸Œë¦¬ë“œ LangGraph ê·¸ë˜í”„ ë¹Œë“œ
    
    Returns:
        CompiledGraph: ì»´íŒŒì¼ëœ ê·¸ë˜í”„
    """
    try:
        from langgraph.graph import StateGraph, END
        
        workflow = StateGraph(HybridState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("typedb_coefficients", typedb_coefficient_node)
        workflow.add_node("pinecone_retrieval", pinecone_retrieval_node)
        workflow.add_node("llm_analysis", llm_analysis_node)
        workflow.add_node("safety_guard", hybrid_safety_guard_node)
        workflow.add_node("report", hybrid_report_node)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("typedb_coefficients")
        workflow.add_edge("typedb_coefficients", "pinecone_retrieval")
        workflow.add_edge("pinecone_retrieval", "llm_analysis")
        workflow.add_edge("llm_analysis", "safety_guard")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        def route_after_safety(state: HybridState) -> str:
            route = state.get("safety_route", "continue")
            if route == "human_escalation":
                return END  # ì¦‰ì‹œ ì¢…ë£Œ (escalation í•„ìš”)
            return "report"
        
        workflow.add_conditional_edges(
            "safety_guard",
            route_after_safety,
            {
                END: END,
                "report": "report",
            }
        )
        
        workflow.add_edge("report", END)
        
        # ì»´íŒŒì¼
        from langgraph.checkpoint.memory import MemorySaver
        graph = workflow.compile(checkpointer=MemorySaver())
        
        logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ LangGraph ê·¸ë˜í”„ ë¹Œë“œ ì™„ë£Œ")
        return graph
        
    except ImportError:
        logger.warning("langgraph íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ì‚¬ìš©.")
        return None


def run_hybrid_workflow(
    user_id: str = "user_ohseho_001",
    goal: str = "HR ì˜¨ë³´ë”© í”„ë¡œì„¸ìŠ¤ ìµœì í™”",
    verbose: bool = True,
) -> HybridState:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    
    Args:
        user_id: ì‚¬ìš©ì ID
        goal: ëª©í‘œ
        verbose: ìƒì„¸ ì¶œë ¥
        
    Returns:
        HybridState: ìµœì¢… ìƒíƒœ
    """
    logger.info(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {goal}")
    
    initial_state: HybridState = {
        "user_id": user_id,
        "goal": goal,
        "messages": [],
    }
    
    graph = build_hybrid_graph()
    
    if graph:
        # LangGraph ì‚¬ìš©
        config = {"configurable": {"thread_id": f"hybrid_{user_id}"}}
        result = graph.invoke(initial_state, config)
    else:
        # Fallback: ìˆœì°¨ ì‹¤í–‰
        logger.info("Fallback ëª¨ë“œ: ìˆœì°¨ ì‹¤í–‰")
        
        state = initial_state
        state.update(typedb_coefficient_node(state))
        state.update(pinecone_retrieval_node(state))
        state.update(llm_analysis_node(state))
        state.update(hybrid_safety_guard_node(state))
        
        if state.get("safety_route") != "human_escalation":
            state.update(hybrid_report_node(state))
        
        result = state
    
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ›ï¸ AUTUS í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ê²°ê³¼")
        print("=" * 60)
        print(f"\nëª©í‘œ: {goal}")
        print(f"Safety Route: {result.get('safety_route')}")
        print(f"ì„±ê³µ: {result.get('success', False)}")
        print(f"\në©”ì‹œì§€:")
        for msg in result.get("messages", []):
            print(f"  - {msg}")
        if result.get("final_report"):
            print(f"\nğŸ“ ìµœì¢… ë³´ê³ ì„œ:\n{result.get('final_report')}")
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _generate_mock_embedding(text: str, dim: int = 1536) -> list[float]:
    """Mock ì„ë² ë”© ìƒì„±"""
    import hashlib
    hash_bytes = hashlib.sha256(text.encode()).digest()
    vector = []
    for i in range(dim):
        byte_val = hash_bytes[i % len(hash_bytes)]
        vector.append((byte_val / 255.0) * 2 - 1)
    return vector


def _hash_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ í•´ì‹œ"""
    import hashlib
    return hashlib.md5(text.encode()).hexdigest()


def _parse_json_response(response: str) -> dict:
    """JSON ì‘ë‹µ íŒŒì‹±"""
    import json
    
    # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
    if "```json" in response:
        start = response.find("```json") + 7
        end = response.find("```", start)
        response = response[start:end].strip()
    elif "```" in response:
        start = response.find("```") + 3
        end = response.find("```", start)
        response = response[start:end].strip()
    
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {"raw": response, "error": "JSON íŒŒì‹± ì‹¤íŒ¨"}
