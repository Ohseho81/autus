"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸŒ AUTUS v2.1 - Humanity Analysis Engine
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ê°œë³„ ì‚¬ìš©ìê°€ ì•„ë‹Œ **ì‚¬ìš©ì ê°„ ìƒí˜¸ê´€ê³„**ë¡œ ì¸ë¥˜ë¥¼ ë¶„ì„

í•µì‹¬ ê°œë…:
  â€¢ ì‚¬ìš©ì ë³€ìˆ˜: ê°œì¸ì˜ 36ê°œ ë…¸ë“œ ê°’
  â€¢ ì—°ê²°ê³ ë¦¬ ë³€ìˆ˜: ë…¸ë“œ ê°„ ì¸ê³¼ê´€ê³„ (ì¸ë¥˜ ê³µí†µ ë²•ì¹™)
  â€¢ ìƒí˜¸ê´€ê³„: ì‚¬ìš©ì ê°„ íŒ¨í„´ â†’ ì¸ë¥˜ ì „ì²´ í†µì°°

ë¶„ì„ ë ˆë²¨:
  1. Individual: ê°œì¸ ì••ë ¥/ìƒíƒœ
  2. Cohort: ìœ ì‚¬ ê·¸ë£¹ í´ëŸ¬ìŠ¤í„°ë§
  3. Humanity: ì „ì²´ ì¸ë¥˜ íŒ¨í„´/ë²•ì¹™ ë°œê²¬
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ Data Structures
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class HumanityInsight:
    """ì¸ë¥˜ ë¶„ì„ ê²°ê³¼"""
    total_users: int
    avg_pressure: float                    # í‰ê·  ì••ë ¥
    pressure_distribution: Dict[str, int]  # ìƒíƒœ ë¶„í¬ (IGNORABLE/PRESSURING/IRREVERSIBLE)
    top_pressures: List[Tuple[str, float]] # ê°€ì¥ ë†’ì€ ì••ë ¥ ë…¸ë“œë“¤
    correlations: Dict[str, float]         # ë…¸ë“œ ê°„ ìƒê´€ê´€ê³„
    clusters: List[Dict]                   # ì¸ë¥˜ ìœ í˜• í´ëŸ¬ìŠ¤í„°
    laws_discovered: List[Dict]            # ë°œê²¬ëœ ì¸ë¥˜ ë²•ì¹™
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass  
class UserVector:
    """ì‚¬ìš©ìë¥¼ 36ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„"""
    user_id: str
    values: np.ndarray      # 36ê°œ ë…¸ë“œ ì›ì‹œê°’
    pressures: np.ndarray   # 36ê°œ ë…¸ë“œ ì••ë ¥ (0-1 ì •ê·œí™”)
    
    @property
    def dominant_layer(self) -> str:
        """ê°€ì¥ ì••ë ¥ì´ ë†’ì€ ë ˆì´ì–´"""
        layer_pressures = {
            'L1_ì¬ë¬´': np.mean(self.pressures[0:8]),
            'L2_ìƒì²´': np.mean(self.pressures[8:14]),
            'L3_ê´€ê³„': np.mean(self.pressures[14:22]),
            'L4_ì‹œê°„': np.mean(self.pressures[22:28]),
            'L5_ì˜ë¯¸': np.mean(self.pressures[28:36]),
        }
        return max(layer_pressures, key=layer_pressures.get)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ Humanity Analysis Engine
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HumanityEngine:
    """ì¸ë¥˜ ìƒí˜¸ê´€ê³„ ë¶„ì„ ì—”ì§„"""
    
    # 36ê°œ ë…¸ë“œ ID
    NODE_IDS = [f"n{i:02d}" for i in range(1, 37)]
    
    # ë ˆì´ì–´ë³„ ë…¸ë“œ ì¸ë±ìŠ¤
    LAYER_INDICES = {
        'L1_ì¬ë¬´': (0, 8),   # n01-n08
        'L2_ìƒì²´': (8, 14),  # n09-n14
        'L3_ê´€ê³„': (14, 22), # n15-n22
        'L4_ì‹œê°„': (22, 28), # n23-n28
        'L5_ì˜ë¯¸': (28, 36), # n29-n36
    }
    
    def __init__(self):
        self.users: Dict[str, UserVector] = {}
        self._correlation_cache: Optional[np.ndarray] = None
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë°ì´í„° ìˆ˜ì§‘
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def add_user(self, user_id: str, node_values: Dict[str, float], 
                 node_pressures: Dict[str, float]) -> None:
        """ì‚¬ìš©ì ì¶”ê°€"""
        values = np.array([node_values.get(nid, 0.0) for nid in self.NODE_IDS])
        pressures = np.array([node_pressures.get(nid, 0.0) for nid in self.NODE_IDS])
        
        self.users[user_id] = UserVector(
            user_id=user_id,
            values=values,
            pressures=pressures
        )
        self._correlation_cache = None  # ìºì‹œ ë¬´íš¨í™”
    
    def add_users_batch(self, users_data: List[Dict]) -> int:
        """ë°°ì¹˜ë¡œ ì‚¬ìš©ì ì¶”ê°€"""
        for data in users_data:
            self.add_user(
                data['user_id'],
                data.get('values', {}),
                data.get('pressures', {})
            )
        return len(users_data)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìƒí˜¸ê´€ê³„ ë¶„ì„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def analyze(self) -> HumanityInsight:
        """ì „ì²´ ì¸ë¥˜ ë¶„ì„"""
        if len(self.users) < 2:
            return self._empty_insight()
        
        # ì••ë ¥ í–‰ë ¬ ìƒì„± (users Ã— 36 nodes)
        pressure_matrix = np.array([u.pressures for u in self.users.values()])
        
        return HumanityInsight(
            total_users=len(self.users),
            avg_pressure=float(np.mean(pressure_matrix)),
            pressure_distribution=self._calc_distribution(pressure_matrix),
            top_pressures=self._find_top_pressures(pressure_matrix),
            correlations=self._calc_correlations(pressure_matrix),
            clusters=self._find_clusters(pressure_matrix),
            laws_discovered=self._discover_laws(pressure_matrix),
        )
    
    def _empty_insight(self) -> HumanityInsight:
        return HumanityInsight(
            total_users=len(self.users),
            avg_pressure=0.0,
            pressure_distribution={},
            top_pressures=[],
            correlations={},
            clusters=[],
            laws_discovered=[],
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë¶„ì„ ë©”ì„œë“œë“¤
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def _calc_distribution(self, matrix: np.ndarray) -> Dict[str, int]:
        """ìƒíƒœ ë¶„í¬ ê³„ì‚°"""
        avg_pressures = np.mean(matrix, axis=1)  # ì‚¬ìš©ìë³„ í‰ê·  ì••ë ¥
        return {
            'IGNORABLE': int(np.sum(avg_pressures < 0.3)),
            'PRESSURING': int(np.sum((avg_pressures >= 0.3) & (avg_pressures < 0.7))),
            'IRREVERSIBLE': int(np.sum(avg_pressures >= 0.7)),
        }
    
    def _find_top_pressures(self, matrix: np.ndarray) -> List[Tuple[str, float]]:
        """ê°€ì¥ ë†’ì€ ì••ë ¥ ë…¸ë“œë“¤"""
        avg_by_node = np.mean(matrix, axis=0)
        top_indices = np.argsort(avg_by_node)[::-1][:5]
        return [(self.NODE_IDS[i], float(avg_by_node[i])) for i in top_indices]
    
    def _calc_correlations(self, matrix: np.ndarray) -> Dict[str, float]:
        """ë…¸ë“œ ê°„ ìƒê´€ê´€ê³„ (ì¸ë¥˜ ê³µí†µ íŒ¨í„´)"""
        if self._correlation_cache is None:
            # ìƒê´€í–‰ë ¬ ê³„ì‚° (36 Ã— 36)
            self._correlation_cache = np.corrcoef(matrix.T)
        
        # ê°•í•œ ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ
        strong_correlations = {}
        for i in range(36):
            for j in range(i + 1, 36):
                corr = self._correlation_cache[i, j]
                if abs(corr) > 0.5:  # ê°•í•œ ìƒê´€ê´€ê³„
                    key = f"{self.NODE_IDS[i]}â†”{self.NODE_IDS[j]}"
                    strong_correlations[key] = round(float(corr), 3)
        
        return dict(sorted(strong_correlations.items(), 
                          key=lambda x: abs(x[1]), reverse=True)[:10])
    
    def _find_clusters(self, matrix: np.ndarray) -> List[Dict]:
        """ì¸ë¥˜ ìœ í˜• í´ëŸ¬ìŠ¤í„°ë§ (K-means ê°„ì†Œí™”)"""
        n_clusters = min(5, len(self.users) // 10 + 1)
        if n_clusters < 2:
            return []
        
        # ê°„ë‹¨í•œ K-means
        centroids = matrix[np.random.choice(len(matrix), n_clusters, replace=False)]
        
        for _ in range(10):  # 10íšŒ ë°˜ë³µ
            # í• ë‹¹
            distances = np.array([[np.linalg.norm(row - c) for c in centroids] 
                                  for row in matrix])
            labels = np.argmin(distances, axis=1)
            
            # ì—…ë°ì´íŠ¸
            new_centroids = np.array([matrix[labels == k].mean(axis=0) 
                                      for k in range(n_clusters)])
            if np.allclose(centroids, new_centroids):
                break
            centroids = new_centroids
        
        # í´ëŸ¬ìŠ¤í„° í•´ì„
        clusters = []
        for k in range(n_clusters):
            mask = labels == k
            cluster_matrix = matrix[mask]
            if len(cluster_matrix) == 0:
                continue
                
            # ì´ í´ëŸ¬ìŠ¤í„°ì˜ íŠ¹ì§• ì°¾ê¸°
            cluster_avg = np.mean(cluster_matrix, axis=0)
            global_avg = np.mean(matrix, axis=0)
            diff = cluster_avg - global_avg
            
            # ê°€ì¥ íŠ¹ì§•ì ì¸ ë…¸ë“œ
            top_idx = np.argmax(np.abs(diff))
            characteristic = self.NODE_IDS[top_idx]
            
            # ì§€ë°°ì  ë ˆì´ì–´ ì°¾ê¸°
            layer_avgs = {
                name: np.mean(cluster_avg[start:end])
                for name, (start, end) in self.LAYER_INDICES.items()
            }
            dominant_layer = max(layer_avgs, key=layer_avgs.get)
            
            clusters.append({
                'id': k,
                'size': int(np.sum(mask)),
                'percentage': round(np.sum(mask) / len(matrix) * 100, 1),
                'avg_pressure': round(float(np.mean(cluster_matrix)), 3),
                'characteristic_node': characteristic,
                'dominant_layer': dominant_layer,
                'name': self._name_cluster(dominant_layer, cluster_avg),
            })
        
        return sorted(clusters, key=lambda x: x['size'], reverse=True)
    
    def _name_cluster(self, layer: str, avg: np.ndarray) -> str:
        """í´ëŸ¬ìŠ¤í„°ì— ì¸ê°„ì  ì´ë¦„ ë¶€ì—¬"""
        names = {
            'L1_ì¬ë¬´': ['ì¬ë¬´ ì••ë°•í˜•', 'ê²½ì œ ë¶ˆì•ˆí˜•', 'ìê¸ˆ ìŠ¤íŠ¸ë ˆìŠ¤í˜•'],
            'L2_ìƒì²´': ['ê±´ê°• ìœ„ê¸°í˜•', 'í”¼ë¡œ ëˆ„ì í˜•', 'ì‹ ì²´ ê²½ê³ í˜•'],
            'L3_ê´€ê³„': ['ê´€ê³„ ê°ˆë“±í˜•', 'ê³ ë¦½ ìœ„í—˜í˜•', 'ì‹ ë¢° ê²°í•í˜•'],
            'L4_ì‹œê°„': ['ì‹œê°„ ê²°í•í˜•', 'ë²ˆì•„ì›ƒ ìœ„í—˜í˜•', 'ê³¼ë¶€í•˜í˜•'],
            'L5_ì˜ë¯¸': ['ëª©ì  ìƒì‹¤í˜•', 'ì˜ë¯¸ ì¶”êµ¬í˜•', 'ë°©í–¥ íƒìƒ‰í˜•'],
        }
        pressure = np.mean(avg)
        idx = 0 if pressure < 0.3 else (1 if pressure < 0.7 else 2)
        return names.get(layer, ['ë¯¸ë¶„ë¥˜'])[min(idx, len(names.get(layer, ['ë¯¸ë¶„ë¥˜'])) - 1)]
    
    def _discover_laws(self, matrix: np.ndarray) -> List[Dict]:
        """ì¸ë¥˜ ê³µí†µ ë²•ì¹™ ë°œê²¬"""
        laws = []
        
        # 1. ë ˆì´ì–´ ê°„ ìƒê´€ê´€ê³„ ë²•ì¹™
        for l1_name, (l1_start, l1_end) in self.LAYER_INDICES.items():
            for l2_name, (l2_start, l2_end) in self.LAYER_INDICES.items():
                if l1_name >= l2_name:
                    continue
                    
                l1_avg = np.mean(matrix[:, l1_start:l1_end], axis=1)
                l2_avg = np.mean(matrix[:, l2_start:l2_end], axis=1)
                corr = np.corrcoef(l1_avg, l2_avg)[0, 1]
                
                if abs(corr) > 0.6:
                    direction = "ì •ë¹„ë¡€" if corr > 0 else "ë°˜ë¹„ë¡€"
                    laws.append({
                        'type': 'layer_correlation',
                        'description': f"{l1_name}ê³¼ {l2_name}ì€ {direction} ê´€ê³„",
                        'correlation': round(float(corr), 3),
                        'confidence': round(abs(float(corr)), 2),
                    })
        
        # 2. ì„ê³„ì  ë²•ì¹™ (íŠ¹ì • ì••ë ¥ ì´ìƒì—ì„œ ì—°ì‡„ ë°˜ì‘)
        high_pressure_users = matrix[np.mean(matrix, axis=1) > 0.6]
        if len(high_pressure_users) > 10:
            cascade_nodes = np.mean(high_pressure_users, axis=0)
            top_cascade = np.argsort(cascade_nodes)[::-1][:3]
            laws.append({
                'type': 'cascade_pattern',
                'description': f"ê³ ì••ë ¥ ìƒíƒœì—ì„œ {', '.join([self.NODE_IDS[i] for i in top_cascade])} ë…¸ë“œê°€ ì—°ì‡„ ìƒìŠ¹",
                'affected_nodes': [self.NODE_IDS[i] for i in top_cascade],
                'confidence': round(len(high_pressure_users) / len(matrix), 2),
            })
        
        # 3. íšŒë³µ íŒ¨í„´ (ì €ì••ë ¥ ì‚¬ìš©ìì˜ ê³µí†µì )
        low_pressure_users = matrix[np.mean(matrix, axis=1) < 0.3]
        if len(low_pressure_users) > 10:
            stable_pattern = np.mean(low_pressure_users, axis=0)
            stable_nodes = np.argsort(stable_pattern)[:3]
            laws.append({
                'type': 'stability_pattern',
                'description': f"ì•ˆì • ìƒíƒœ ì‚¬ìš©ìëŠ” {', '.join([self.NODE_IDS[i] for i in stable_nodes])} ë…¸ë“œê°€ ë‚®ìŒ",
                'stable_nodes': [self.NODE_IDS[i] for i in stable_nodes],
                'confidence': round(len(low_pressure_users) / len(matrix), 2),
            })
        
        return sorted(laws, key=lambda x: x['confidence'], reverse=True)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íŠ¹ìˆ˜ ë¶„ì„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def find_similar_users(self, user_id: str, top_n: int = 5) -> List[Tuple[str, float]]:
        """ìœ ì‚¬í•œ ì‚¬ìš©ì ì°¾ê¸°"""
        if user_id not in self.users:
            return []
        
        target = self.users[user_id].pressures
        similarities = []
        
        for uid, user in self.users.items():
            if uid == user_id:
                continue
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            sim = np.dot(target, user.pressures) / (
                np.linalg.norm(target) * np.linalg.norm(user.pressures) + 1e-9
            )
            similarities.append((uid, float(sim)))
        
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]
    
    def predict_future(self, user_id: str) -> Dict:
        """ìœ ì‚¬ ì‚¬ìš©ì ê¸°ë°˜ ë¯¸ë˜ ì˜ˆì¸¡"""
        similar = self.find_similar_users(user_id, top_n=10)
        if not similar:
            return {}
        
        # ìœ ì‚¬ ì‚¬ìš©ìë“¤ì˜ ì••ë ¥ í‰ê· 
        similar_pressures = np.mean([
            self.users[uid].pressures for uid, _ in similar
        ], axis=0)
        
        target = self.users[user_id].pressures
        trend = similar_pressures - target
        
        # ìƒìŠ¹ ì˜ˆìƒ ë…¸ë“œ
        rising = np.argsort(trend)[::-1][:3]
        falling = np.argsort(trend)[:3]
        
        return {
            'rising_risk': [(self.NODE_IDS[i], round(float(trend[i]), 3)) for i in rising],
            'falling_risk': [(self.NODE_IDS[i], round(float(trend[i]), 3)) for i in falling],
            'similar_users_count': len(similar),
        }
    
    def get_humanity_health(self) -> Dict:
        """ì¸ë¥˜ ì „ì²´ ê±´ê°•ë„"""
        if len(self.users) < 1:
            return {'health_score': 0, 'status': 'NO_DATA'}
        
        all_pressures = np.array([u.pressures for u in self.users.values()])
        avg_pressure = np.mean(all_pressures)
        
        # ê±´ê°• ì ìˆ˜ (ì••ë ¥ì´ ë‚®ì„ìˆ˜ë¡ ê±´ê°•)
        health_score = round((1 - avg_pressure) * 100, 1)
        
        # ìƒíƒœ íŒë‹¨
        if health_score >= 70:
            status = 'HEALTHY'
        elif health_score >= 40:
            status = 'STRESSED'
        else:
            status = 'CRITICAL'
        
        # ë ˆì´ì–´ë³„ ê±´ê°•ë„
        layer_health = {}
        for name, (start, end) in self.LAYER_INDICES.items():
            layer_avg = np.mean(all_pressures[:, start:end])
            layer_health[name] = round((1 - layer_avg) * 100, 1)
        
        return {
            'health_score': health_score,
            'status': status,
            'total_users': len(self.users),
            'layer_health': layer_health,
            'weakest_layer': min(layer_health, key=layer_health.get),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ Global Instance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

humanity_engine = HumanityEngine()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import random
    
    print("=" * 70)
    print("ğŸŒ Humanity Analysis Engine í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    engine = HumanityEngine()
    
    # ì‹œë®¬ë ˆì´ì…˜: 1000ëª…ì˜ ë‹¤ì–‘í•œ ì‚¬ìš©ì ìƒì„±
    print("\nğŸ“Š 1,000ëª… ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜...")
    
    for i in range(1000):
        # ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì‚¬ìš©ì ìƒì„±
        user_type = random.choice(['healthy', 'stressed', 'burnout', 'financial', 'relationship'])
        
        base_pressure = {
            'healthy': 0.2,
            'stressed': 0.5,
            'burnout': 0.8,
            'financial': 0.3,
            'relationship': 0.4,
        }[user_type]
        
        # ìœ í˜•ë³„ íŠ¹í™” ì••ë ¥
        pressures = {}
        for j, nid in enumerate(engine.NODE_IDS):
            noise = random.gauss(0, 0.15)
            if user_type == 'financial' and j < 8:
                pressures[nid] = min(1, max(0, 0.7 + noise))
            elif user_type == 'burnout' and 8 <= j < 14:
                pressures[nid] = min(1, max(0, 0.8 + noise))
            elif user_type == 'relationship' and 14 <= j < 22:
                pressures[nid] = min(1, max(0, 0.6 + noise))
            else:
                pressures[nid] = min(1, max(0, base_pressure + noise))
        
        engine.add_user(f"user_{i}", {}, pressures)
    
    # ë¶„ì„ ì‹¤í–‰
    print("\nğŸ”¬ ì¸ë¥˜ ë¶„ì„ ì‹¤í–‰...")
    insight = engine.analyze()
    
    print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
    print(f"   â€¢ ì´ ì‚¬ìš©ì: {insight.total_users:,}ëª…")
    print(f"   â€¢ í‰ê·  ì••ë ¥: {insight.avg_pressure:.3f}")
    print(f"\n   ìƒíƒœ ë¶„í¬:")
    for state, count in insight.pressure_distribution.items():
        pct = count / insight.total_users * 100
        bar = "â–ˆ" * int(pct / 5)
        print(f"     {state:15} {count:>4}ëª… ({pct:>5.1f}%) {bar}")
    
    print(f"\n   ğŸ”¥ ê³ ì••ë ¥ ë…¸ë“œ TOP 5:")
    for node, pressure in insight.top_pressures:
        print(f"     {node}: {pressure:.3f}")
    
    print(f"\n   ğŸ”— ê°•í•œ ìƒê´€ê´€ê³„ (ì¸ë¥˜ ê³µí†µ):")
    for pair, corr in list(insight.correlations.items())[:5]:
        direction = "â†‘â†‘" if corr > 0 else "â†‘â†“"
        print(f"     {pair} {direction} r={corr}")
    
    print(f"\n   ğŸ‘¥ ì¸ë¥˜ ìœ í˜• í´ëŸ¬ìŠ¤í„°:")
    for cluster in insight.clusters:
        print(f"     [{cluster['name']}] {cluster['size']}ëª… ({cluster['percentage']}%)")
        print(f"        íŠ¹ì§• ë…¸ë“œ: {cluster['characteristic_node']}, í‰ê· ì••ë ¥: {cluster['avg_pressure']}")
    
    print(f"\n   ğŸ“œ ë°œê²¬ëœ ì¸ë¥˜ ë²•ì¹™:")
    for law in insight.laws_discovered[:3]:
        print(f"     â€¢ {law['description']}")
        print(f"       (ì‹ ë¢°ë„: {law['confidence']})")
    
    # ì¸ë¥˜ ê±´ê°•ë„
    health = engine.get_humanity_health()
    print(f"\n   ğŸŒ ì¸ë¥˜ ê±´ê°•ë„: {health['health_score']}ì  [{health['status']}]")
    print(f"      ê°€ì¥ ì•½í•œ ì˜ì—­: {health['weakest_layer']}")
    
    print("\n" + "=" * 70)
