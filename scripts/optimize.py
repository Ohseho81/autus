#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ AUTUS Optimizer v1.0
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ìë™ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸:
- ì¤‘ë³µ íŒŒì¼ íƒì§€ ë° ì‚­ì œ
- ìºì‹œ ì •ë¦¬
- ì½”ë“œ ë¦°íŠ¸
- ë¹Œë“œ ìµœì í™”

ì‚¬ìš©ë²•: python scripts/optimize.py [--dry-run] [--full]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
import os
import sys
import shutil
import hashlib
import argparse
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
ROOT = Path(__file__).parent.parent
BACKEND = ROOT / "backend"
FRONTEND = ROOT / "frontend"
DEPLOY = FRONTEND / "deploy"

# ë¬´ì‹œ íŒ¨í„´
IGNORE_DIRS = {".git", ".venv", "venv", "node_modules", "__pycache__", ".pytest_cache", ".mypy_cache"}
IGNORE_FILES = {".DS_Store", ".gitkeep", "*.pyc"}


def find_duplicates(directory: Path) -> Dict[str, List[Path]]:
    """ë™ì¼ í•´ì‹œ íŒŒì¼ íƒì§€"""
    hashes = defaultdict(list)
    
    for path in directory.rglob("*"):
        if path.is_file():
            if any(ignored in path.parts for ignored in IGNORE_DIRS):
                continue
            try:
                content = path.read_bytes()
                h = hashlib.md5(content).hexdigest()
                hashes[h].append(path)
            except Exception:
                pass
    
    return {k: v for k, v in hashes.items() if len(v) > 1}


def clean_pycache(directory: Path, dry_run: bool = True) -> int:
    """__pycache__ ì •ë¦¬"""
    count = 0
    for path in directory.rglob("__pycache__"):
        if path.is_dir():
            print(f"  ğŸ—‘ï¸ {path.relative_to(ROOT)}")
            if not dry_run:
                shutil.rmtree(path)
            count += 1
    return count


def clean_empty_dirs(directory: Path, dry_run: bool = True) -> int:
    """ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
    count = 0
    for path in sorted(directory.rglob("*"), key=lambda p: -len(p.parts)):
        if path.is_dir() and not any(path.iterdir()):
            if any(ignored in path.parts for ignored in IGNORE_DIRS):
                continue
            print(f"  ğŸ“‚ (empty) {path.relative_to(ROOT)}")
            if not dry_run:
                path.rmdir()
            count += 1
    return count


def analyze_frontend_html() -> Dict[str, int]:
    """Frontend HTML íŒŒì¼ ë¶„ì„"""
    stats = {}
    if DEPLOY.exists():
        for html in DEPLOY.glob("*.html"):
            size = html.stat().st_size
            stats[html.name] = size
    return dict(sorted(stats.items(), key=lambda x: -x[1]))


def analyze_backend_api() -> List[str]:
    """Backend API íŒŒì¼ ë¶„ì„"""
    api_dir = BACKEND / "api"
    apis = []
    if api_dir.exists():
        for py in api_dir.glob("*.py"):
            if py.name != "__init__.py":
                apis.append(py.stem)
    return sorted(apis)


def run_optimization(dry_run: bool = True, full: bool = False):
    """ìµœì í™” ì‹¤í–‰"""
    print()
    print("â•" * 60)
    print("  ğŸ”§ AUTUS Optimizer v1.0")
    print("â•" * 60)
    print()
    
    mode = "DRY-RUN" if dry_run else "EXECUTE"
    print(f"  Mode: {mode}")
    print()
    
    # 1. ì¤‘ë³µ íŒŒì¼ íƒì§€
    print("ğŸ“ ì¤‘ë³µ íŒŒì¼ íƒì§€...")
    duplicates = find_duplicates(ROOT)
    if duplicates:
        for h, paths in duplicates.items():
            print(f"  Hash {h[:8]}...:")
            for p in paths:
                print(f"    - {p.relative_to(ROOT)}")
    else:
        print("  âœ… ì¤‘ë³µ íŒŒì¼ ì—†ìŒ")
    print()
    
    # 2. ìºì‹œ ì •ë¦¬
    print("ğŸ§¹ ìºì‹œ ì •ë¦¬...")
    cache_count = clean_pycache(ROOT, dry_run)
    print(f"  ì´ {cache_count}ê°œ __pycache__ í´ë”")
    print()
    
    # 3. ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if full:
        print("ğŸ“‚ ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬...")
        empty_count = clean_empty_dirs(ROOT, dry_run)
        print(f"  ì´ {empty_count}ê°œ ë¹ˆ í´ë”")
        print()
    
    # 4. Frontend HTML ë¶„ì„
    print("ğŸ“Š Frontend HTML íŒŒì¼ í¬ê¸°:")
    html_stats = analyze_frontend_html()
    total_size = 0
    for name, size in html_stats.items():
        kb = size / 1024
        print(f"  {name:30} {kb:6.1f} KB")
        total_size += size
    print(f"  {'â”€' * 40}")
    print(f"  {'Total':30} {total_size/1024:6.1f} KB")
    print()
    
    # 5. Backend API ëª©ë¡
    print("ğŸ”Œ Backend API ëª¨ë“ˆ:")
    apis = analyze_backend_api()
    print(f"  ì´ {len(apis)}ê°œ API")
    for api in apis:
        print(f"    - {api}")
    print()
    
    # ê²°ê³¼
    print("â•" * 60)
    if dry_run:
        print("  ğŸ’¡ ì‹¤ì œ ì‹¤í–‰: python scripts/optimize.py --execute")
    else:
        print("  âœ… ìµœì í™” ì™„ë£Œ!")
    print("â•" * 60)
    print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AUTUS Optimizer")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤í–‰í•˜ì§€ ì•Šê³  ë¯¸ë¦¬ë³´ê¸°ë§Œ")
    parser.add_argument("--execute", action="store_true", help="ì‹¤ì œ ì‹¤í–‰")
    parser.add_argument("--full", action="store_true", help="ì „ì²´ ìµœì í™” (ë¹ˆ í´ë” í¬í•¨)")
    
    args = parser.parse_args()
    
    dry_run = not args.execute
    run_optimization(dry_run=dry_run, full=args.full)
