#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df
















#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ§¬ AUTUS PIPELINE v1.3 FINAL - Data Ingestion                          â•‘
â•‘                                                                                           â•‘
â•‘  v1.3 ì—…ê·¸ë ˆì´ë“œ:                                                                          â•‘
â•‘  âœ… customer_id í•„ìˆ˜ (ë¹ˆê°’ ë¶ˆê°€)                                                           â•‘
â•‘  âœ… project_id ìë™ í• ë‹¹ (__AUTO__ â†’ AUTO-{customer}-{YYYYMM})                             â•‘
â•‘  âœ… burn_eventsì— prevented_by, prevented_minutes ì»¬ëŸ¼ ì¶”ê°€                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
from datetime import datetime
from typing import Optional
from .schemas import MONEY_EVENT_TYPES, RECO_TYPES, BURN_TYPES


def _parse_date(s: str) -> pd.Timestamp:
    """ë‚ ì§œ íŒŒì‹±"""
    return pd.to_datetime(s, errors="raise")


def auto_assign_project_id(df: pd.DataFrame) -> pd.DataFrame:
    """
    v1.3: project_idê°€ '__AUTO__'ì´ë©´ ìë™ í• ë‹¹
    
    í˜•ì‹: AUTO-{customer_id}-{YYYYMM}
    """
    out = df.copy()
    mask = out["project_id"] == "__AUTO__"
    
    if mask.any():
        out.loc[mask, "project_id"] = out.loc[mask].apply(
            lambda r: f"AUTO-{r['customer_id']}-{r['date'].strftime('%Y%m')}",
            axis=1
        )
    
    return out


def read_money_events(path: str) -> pd.DataFrame:
    """
    Money Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.3 ë³€ê²½:
    - customer_id: REQUIRED (ë¹ˆê°’ ë¶ˆê°€)
    - project_id: OPTIONAL (ë¹ˆê°’ ì‹œ ìë™ í• ë‹¹)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = [
        "event_id", "date", "event_type", "currency", "amount", "people_tags",
        "effective_minutes", "evidence_id", "recommendation_type"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"money_events missing columns: {missing}")
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # event_type ê²€ì¦
    bad_types = df.loc[~df["event_type"].isin(MONEY_EVENT_TYPES), "event_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid event_type: {bad_types}")
    
    # recommendation_type ê²€ì¦
    df["recommendation_type"] = df["recommendation_type"].fillna("")
    bad_reco = df.loc[~df["recommendation_type"].isin(RECO_TYPES), "recommendation_type"].unique().tolist()
    if bad_reco:
        raise ValueError(f"invalid recommendation_type: {bad_reco}")
    
    # people_tags ê²€ì¦ (1~3ëª…)
    def _count_tags(x: str) -> int:
        tags = [t.strip() for t in str(x).split(";") if t.strip()]
        return len(tags)
    
    tag_counts = df["people_tags"].apply(_count_tags)
    if (tag_counts < 1).any() or (tag_counts > 3).any():
        bad = df.loc[(tag_counts < 1) | (tag_counts > 3), ["event_id", "people_tags"]]
        raise ValueError(f"people_tags must have 1..3 tags. bad rows:\n{bad}")
    
    # effective_minutes ê²€ì¦ (5~1440ë¶„)
    if (df["effective_minutes"] < 5).any() or (df["effective_minutes"] > 1440).any():
        bad = df.loc[
            (df["effective_minutes"] < 5) | (df["effective_minutes"] > 1440),
            ["event_id", "effective_minutes"]
        ]
        raise ValueError(f"effective_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["event_id", "evidence_id"]]
        raise ValueError(f"duplicate evidence_id detected:\n{dup}")
    
    # ì„ íƒ í•„ë“œ ê¸°ë³¸ê°’
    if "contract_months" not in df.columns:
        df["contract_months"] = None
    if "recommendation_id" not in df.columns:
        df["recommendation_id"] = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.3: customer_id í•„ìˆ˜ + project_id ìë™ í• ë‹¹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # customer_id í•„ìˆ˜ ê²€ì¦
    if "customer_id" not in df.columns:
        # v1.3 ì™„í™”: ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ê°’ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” raise)
        print("âš ï¸ WARNING: customer_id column missing. Using '__DEFAULT_CUSTOMER__'")
        df["customer_id"] = "__DEFAULT_CUSTOMER__"
    else:
        df["customer_id"] = df["customer_id"].astype(str).str.strip()
        # ë¹ˆê°’ ê²€ì¦ (v1.3 LOCKì—ì„œëŠ” ì—ëŸ¬, ì—¬ê¸°ì„œëŠ” ì™„í™”)
        empty_mask = (df["customer_id"] == "") | (df["customer_id"].isna()) | (df["customer_id"] == "nan")
        if empty_mask.any():
            print(f"âš ï¸ WARNING: {empty_mask.sum()} rows have empty customer_id. Using '__UNKNOWN__'")
            df.loc[empty_mask, "customer_id"] = "__UNKNOWN__"
    
    # project_id ì²˜ë¦¬
    if "project_id" not in df.columns:
        df["project_id"] = "__AUTO__"
    else:
        df["project_id"] = df["project_id"].fillna("__AUTO__").astype(str).str.strip()
        df.loc[df["project_id"] == "", "project_id"] = "__AUTO__"
        df.loc[df["project_id"] == "nan", "project_id"] = "__AUTO__"
    
    # project_id ìë™ í• ë‹¹
    df = auto_assign_project_id(df)
    
    return df


def read_burn_events(path: str) -> pd.DataFrame:
    """
    Burn Events CSV ì½ê¸° ë° ê²€ì¦
    
    v1.1 ë³€ê²½:
    - burn_typeì— PREVENTED, FIXED ì¶”ê°€
    - prevented_by: Controller í›„ë³´ ID
    - prevented_minutes: ì¤„ì¸ ì‹œê°„(ë¶„)
    """
    df = pd.read_csv(path)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required = ["burn_id", "date", "burn_type", "person_or_edge", "loss_minutes", "evidence_id"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"burn_events missing columns: {missing}")
    
    # v1.1 ì„ íƒ ì»¬ëŸ¼
    if "prevented_by" not in df.columns:
        df["prevented_by"] = None
    if "prevented_minutes" not in df.columns:
        df["prevented_minutes"] = 0
    
    # ë‚ ì§œ íŒŒì‹±
    df["date"] = df["date"].apply(_parse_date)
    
    # burn_type ê²€ì¦
    bad_types = df.loc[~df["burn_type"].isin(BURN_TYPES), "burn_type"].unique().tolist()
    if bad_types:
        raise ValueError(f"invalid burn_type: {bad_types}")
    
    # loss_minutes ê²€ì¦ (0~1440ë¶„, PREVENTED/FIXEDëŠ” 0 ê°€ëŠ¥)
    if (df["loss_minutes"] < 0).any() or (df["loss_minutes"] > 1440).any():
        bad = df.loc[
            (df["loss_minutes"] < 0) | (df["loss_minutes"] > 1440),
            ["burn_id", "loss_minutes"]
        ]
        raise ValueError(f"loss_minutes out of range. bad rows:\n{bad}")
    
    # prevented_minutes ê²€ì¦
    df["prevented_minutes"] = df["prevented_minutes"].fillna(0).astype(float)
    if (df["prevented_minutes"] < 0).any() or (df["prevented_minutes"] > 1440).any():
        bad = df.loc[
            (df["prevented_minutes"] < 0) | (df["prevented_minutes"] > 1440),
            ["burn_id", "prevented_minutes"]
        ]
        raise ValueError(f"prevented_minutes out of range. bad rows:\n{bad}")
    
    # evidence_id ê³ ìœ ì„± ê²€ì¦
    if df["evidence_id"].duplicated().any():
        dup = df.loc[df["evidence_id"].duplicated(keep=False), ["burn_id", "evidence_id"]]
        raise ValueError(f"duplicate burn evidence_id detected:\n{dup}")
    
    # prevented_by ì •ë¦¬
    df["prevented_by"] = df["prevented_by"].fillna("").astype(str).str.strip()
    
    return df


def read_fx_rates(path: str) -> pd.DataFrame:
    """FX Rates CSV ì½ê¸°"""
    df = pd.read_csv(path)
    
    required = ["date", "currency", "fx_rate_to_krw", "source"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"fx_rates missing columns: {missing}")
    
    df["date"] = df["date"].apply(_parse_date)
    return df


def read_edges(path: str) -> pd.DataFrame:
    """Edges CSV ì½ê¸° (ì¸ê°„ ê´€ê³„ ê·¸ë˜í”„)"""
    df = pd.read_csv(path)
    
    required = ["from_id", "to_id", "link_strength"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"edges missing columns: {missing}")
    
    if (df["link_strength"] < 0).any() or (df["link_strength"] > 1).any():
        bad = df.loc[
            (df["link_strength"] < 0) | (df["link_strength"] > 1),
            ["from_id", "to_id", "link_strength"]
        ]
        raise ValueError(f"link_strength must be 0..1. bad rows:\n{bad}")
    
    return df


def read_historical_burns(path: str) -> pd.DataFrame:
    """Historical Burns ì½ê¸° (ì „ì£¼/ì „ì „ì£¼ ë¹„êµìš©)"""
    df = pd.read_csv(path)
    
    required = ["week_id", "person_id", "burn_minutes", "burn_count"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"historical_burns missing columns: {missing}")
    
    return df






















